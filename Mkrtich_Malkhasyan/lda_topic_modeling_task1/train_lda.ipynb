{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f30431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec30405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    tokens = [\n",
    "        token for token in tokens\n",
    "        if token not in STOPWORDS and len(token) >= 3\n",
    "    ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d167e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 1000 documents\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Function\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "documents = newsgroups.data[:1000]\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e439d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Documents\n",
    "processed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcae155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2540\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary & Filter Words\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "dictionary.filter_extremes(\n",
    "    no_below=5,     # appears in at least 5 docs\n",
    "    no_above=0.5    # appears in <= 50% docs\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac0b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BoW Corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef97a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA Model\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    "    passes=15,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76ca7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save Model & Dictionary\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "lda_model.save(\"models/lda_model\")\n",
    "dictionary.save(\"models/dictionary.dict\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15b08cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "0.009*\"greek\" + 0.008*\"gun\" + 0.007*\"nasa\" + 0.007*\"data\" + 0.007*\"state\" + 0.006*\"space\" + 0.006*\"time\" + 0.006*\"think\" + 0.005*\"government\" + 0.005*\"use\" + 0.005*\"people\" + 0.005*\"going\" + 0.004*\"new\" + 0.004*\"guns\" + 0.004*\"way\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 1\n",
      "0.014*\"argument\" + 0.012*\"true\" + 0.011*\"god\" + 0.011*\"example\" + 0.011*\"know\" + 0.009*\"truth\" + 0.008*\"believe\" + 0.007*\"conclusion\" + 0.007*\"son\" + 0.007*\"people\" + 0.007*\"father\" + 0.007*\"spirit\" + 0.006*\"bible\" + 0.006*\"way\" + 0.006*\"like\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 2\n",
      "0.025*\"armenian\" + 0.020*\"turkish\" + 0.017*\"armenians\" + 0.015*\"genocide\" + 0.013*\"people\" + 0.010*\"soviet\" + 0.009*\"russian\" + 0.009*\"turks\" + 0.008*\"war\" + 0.008*\"government\" + 0.008*\"killed\" + 0.007*\"jews\" + 0.007*\"muslim\" + 0.006*\"population\" + 0.006*\"army\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 3\n",
      "0.055*\"max\" + 0.018*\"use\" + 0.011*\"health\" + 0.008*\"software\" + 0.007*\"files\" + 0.006*\"disk\" + 0.006*\"like\" + 0.006*\"case\" + 0.005*\"problem\" + 0.005*\"memory\" + 0.005*\"need\" + 0.005*\"file\" + 0.005*\"ram\" + 0.004*\"image\" + 0.004*\"simms\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 4\n",
      "0.017*\"windows\" + 0.011*\"thanks\" + 0.010*\"edu\" + 0.009*\"know\" + 0.009*\"com\" + 0.009*\"like\" + 0.007*\"program\" + 0.007*\"scsi\" + 0.006*\"new\" + 0.006*\"mail\" + 0.006*\"chip\" + 0.006*\"line\" + 0.005*\"information\" + 0.005*\"want\" + 0.005*\"problem\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 5\n",
      "0.016*\"people\" + 0.009*\"said\" + 0.008*\"think\" + 0.007*\"like\" + 0.007*\"man\" + 0.006*\"know\" + 0.005*\"right\" + 0.005*\"got\" + 0.005*\"came\" + 0.005*\"year\" + 0.005*\"father\" + 0.004*\"armenians\" + 0.004*\"long\" + 0.004*\"left\" + 0.004*\"started\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 6\n",
      "0.024*\"jesus\" + 0.011*\"matthew\" + 0.009*\"space\" + 0.007*\"said\" + 0.006*\"shuttle\" + 0.006*\"church\" + 0.006*\"radius\" + 0.005*\"earth\" + 0.005*\"nasa\" + 0.005*\"launch\" + 0.004*\"like\" + 0.004*\"orbit\" + 0.004*\"christian\" + 0.004*\"know\" + 0.004*\"lord\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 7\n",
      "0.023*\"good\" + 0.011*\"game\" + 0.010*\"excellent\" + 0.008*\"games\" + 0.008*\"missing\" + 0.007*\"team\" + 0.007*\"bit\" + 0.006*\"people\" + 0.006*\"runs\" + 0.006*\"time\" + 0.006*\"like\" + 0.006*\"fair\" + 0.005*\"cover\" + 0.005*\"thanks\" + 0.005*\"poster\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 8\n",
      "0.016*\"people\" + 0.012*\"god\" + 0.012*\"period\" + 0.011*\"time\" + 0.011*\"think\" + 0.009*\"know\" + 0.009*\"play\" + 0.008*\"power\" + 0.007*\"problem\" + 0.007*\"jesus\" + 0.007*\"second\" + 0.006*\"things\" + 0.006*\"way\" + 0.006*\"like\" + 0.005*\"king\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Topic 9\n",
      "0.008*\"car\" + 0.008*\"insurance\" + 0.007*\"think\" + 0.007*\"year\" + 0.007*\"like\" + 0.006*\"mark\" + 0.006*\"water\" + 0.006*\"years\" + 0.006*\"israel\" + 0.006*\"said\" + 0.006*\"sin\" + 0.005*\"good\" + 0.005*\"time\" + 0.005*\"god\" + 0.004*\"group\"\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print Discovered Topics\n",
    "for i, topic in lda_model.print_topics(num_words=15):\n",
    "    print(f\"\\nTopic {i}\")\n",
    "    print(topic)\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
