{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0554b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 300\n",
      "\n",
      "First 30 vocabulary entries:\n",
      "0 <unk>\n",
      "1 <s>\n",
      "2 </s>\n",
      "3 ու\n",
      "4 ան\n",
      "5 այ\n",
      "6 եր\n",
      "7 ար\n",
      "8 ուն\n",
      "9 ▁հ\n",
      "10 ում\n",
      "11 ակ\n",
      "12 ութ\n",
      "13 ▁է\n",
      "14 ությ\n",
      "15 են\n",
      "16 ություն\n",
      "17 ▁Հ\n",
      "18 ներ\n",
      "19 աս\n",
      "20 ▁Հայ\n",
      "21 ▁կ\n",
      "22 որ\n",
      "23 ամ\n",
      "24 ական\n",
      "25 եւ\n",
      "26 ատ\n",
      "27 ▁են\n",
      "28 ▁մ\n",
      "29 ▁հայ\n",
      "\n",
      "Last 30 vocabulary entries:\n",
      "270 Բ\n",
      "271 չ\n",
      "272 ջ\n",
      "273 փ\n",
      "274 Կ\n",
      "275 Ն\n",
      "276 Տ\n",
      "277 ձ\n",
      "278 Ե\n",
      "279 Մ\n",
      "280 Գ\n",
      "281 Դ\n",
      "282 Ծ\n",
      "283 Պ\n",
      "284 օ\n",
      "285 Թ\n",
      "286 Լ\n",
      "287 Խ\n",
      "288 Շ\n",
      "289 Ռ\n",
      "290 Ս\n",
      "291 Վ\n",
      "292 Ֆ\n",
      "293 ,\n",
      "294 Ը\n",
      "295 Ի\n",
      "296 Ձ\n",
      "297 Ղ\n",
      "298 Ո\n",
      "299 Ջ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: hy_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 300\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 93 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=4406\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=66\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 93 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 93\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 332\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=230 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=20 all=909 active=843 piece=որ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=1076 active=1010 piece=ները\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=60 all=1198 active=1132 piece=իտ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=80 all=1319 active=1253 piece=արգ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1405 active=1339 piece=▁եր\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1484 active=1077 piece=շակ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=140 all=1534 active=1127 piece=▁Մ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=160 all=1562 active=1155 piece=▁ոլոր\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=180 all=1593 active=1186 piece=մբ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=200 all=1646 active=1239 piece=րագ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1661 active=1012 piece=▁հետազոտ\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: hy_bpe.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: hy_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Training Script (using existing corpus.txt)\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece BPE model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"corpus.txt\",       \n",
    "    model_prefix=\"hy_bpe\",\n",
    "    vocab_size=300,\n",
    "    model_type=\"bpe\",\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hy_bpe.model\")\n",
    "\n",
    "# Vocabulary statistics\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(\"Total vocabulary size:\", vocab_size)\n",
    "\n",
    "print(\"\\nFirst 30 vocabulary entries:\")\n",
    "for i in range(30):\n",
    "    print(i, sp.id_to_piece(i))\n",
    "\n",
    "print(\"\\nLast 30 vocabulary entries:\")\n",
    "for i in range(vocab_size - 30, vocab_size):\n",
    "    print(i, sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da0676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S1: Հայաստանն ունի հարուստ պատմություն։\n",
      "Token pieces: ['▁Հայաստան', 'ն', '▁ունի', '▁հարուստ', '▁պ', 'ատ', 'մ', 'ություն', '։']\n",
      "Token IDs: [35, 236, 60, 221, 95, 26, 242, 16, 246]\n",
      "Decoded: Հայաստանն ունի հարուստ պատմություն։\n",
      "Matches original: True\n",
      "\n",
      "S2: Արհեստական բանականությունը արագ զարգանում է։\n",
      "Token pieces: ['▁Ար', 'հ', 'եստ', 'ական', '▁բ', 'ան', 'ականությունը', '▁արագ', '▁զարգ', 'անում', '▁է', '։']\n",
      "Token IDs: [149, 247, 98, 24, 56, 4, 229, 161, 132, 158, 13, 246]\n",
      "Decoded: Արհեստական բանականությունը արագ զարգանում է։\n",
      "Matches original: True\n",
      "\n",
      "S3: Ծրագրավորումը կարևոր հմտություն է ապագայի համար։\n",
      "Token pieces: ['▁Ծ', 'րագ', 'րա', 'վ', 'որ', 'ումը', '▁կարեւոր', '▁հ', 'մ', 'տ', 'ություն', '▁է', '▁ապագայի', '▁համար', '։']\n",
      "Token IDs: [188, 202, 73, 252, 22, 208, 40, 9, 242, 244, 16, 13, 220, 165, 246]\n",
      "Decoded: Ծրագրավորումը կարեւոր հմտություն է ապագայի համար։\n",
      "Matches original: False\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Encoding and Decoding Script\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hy_bpe.model\")\n",
    "\n",
    "# Test sentences\n",
    "sentences = {\n",
    "    \"S1\": \"Հայաստանն ունի հարուստ պատմություն։\",\n",
    "    \"S2\": \"Արհեստական բանականությունը արագ զարգանում է։\",\n",
    "    \"S3\": \"Ծրագրավորումը կարևոր հմտություն է ապագայի համար։\"\n",
    "}\n",
    "\n",
    "for name, sentence in sentences.items():\n",
    "    print(f\"\\n{name}: {sentence}\")\n",
    "    \n",
    "    # Encode to token pieces\n",
    "    pieces = sp.encode(sentence, out_type=str)\n",
    "    print(\"Token pieces:\", pieces)\n",
    "    \n",
    "    # Encode to token IDs\n",
    "    ids = sp.encode(sentence, out_type=int)\n",
    "    print(\"Token IDs:\", ids)\n",
    "    \n",
    "    # Decode back\n",
    "    decoded = sp.decode(ids)\n",
    "    print(\"Decoded:\", decoded)\n",
    "    \n",
    "    # Check equality\n",
    "    print(\"Matches original:\", decoded == sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f4985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Armenian characters: 100\n",
      "Subword fragments (2–4 chars): 158\n",
      "Full words (5+ chars): 41\n",
      "\n",
      "Top 10 most frequent token pieces:\n",
      "։ -> 93\n",
      "▁է -> 53\n",
      "▁ -> 41\n",
      "ն -> 35\n",
      "ան -> 33\n",
      "ի -> 33\n",
      "▁են -> 27\n",
      "ը -> 25\n",
      "ր -> 22\n",
      "ում -> 20\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Vocabulary Analysis Script\n",
    "\n",
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hy_bpe.model\")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "single_chars = 0\n",
    "subwords = 0\n",
    "full_words = 0\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    piece = sp.id_to_piece(i)\n",
    "    \n",
    "    # Remove SentencePiece whitespace marker\n",
    "    clean_piece = piece.replace(\"▁\", \"\")\n",
    "    length = len(clean_piece)\n",
    "    \n",
    "    if length == 1:\n",
    "        single_chars += 1\n",
    "    elif 2 <= length <= 4:\n",
    "        subwords += 1\n",
    "    elif length >= 5:\n",
    "        full_words += 1\n",
    "\n",
    "print(\"Single Armenian characters:\", single_chars)\n",
    "print(\"Subword fragments (2–4 chars):\", subwords)\n",
    "print(\"Full words (5+ chars):\", full_words)\n",
    "\n",
    "# Read entire corpus from file\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Encode full corpus\n",
    "all_pieces = sp.encode(corpus, out_type=str)\n",
    "counter = Counter(all_pieces)\n",
    "\n",
    "print(\"\\nTop 10 most frequent token pieces:\")\n",
    "for piece, freq in counter.most_common(10):\n",
    "    print(piece, \"->\", freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
