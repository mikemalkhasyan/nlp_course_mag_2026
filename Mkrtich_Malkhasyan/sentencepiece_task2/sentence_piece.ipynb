{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0554b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 300\n",
      "\n",
      "First 30 vocabulary entries:\n",
      "0 <unk>\n",
      "1 <s>\n",
      "2 </s>\n",
      "3 ու\n",
      "4 ան\n",
      "5 այ\n",
      "6 եր\n",
      "7 ար\n",
      "8 ուն\n",
      "9 ▁հ\n",
      "10 ում\n",
      "11 ակ\n",
      "12 ութ\n",
      "13 ▁է\n",
      "14 ությ\n",
      "15 են\n",
      "16 ություն\n",
      "17 ▁Հ\n",
      "18 ներ\n",
      "19 աս\n",
      "20 ▁Հայ\n",
      "21 ▁կ\n",
      "22 որ\n",
      "23 ամ\n",
      "24 ական\n",
      "25 եւ\n",
      "26 ատ\n",
      "27 ▁են\n",
      "28 ▁մ\n",
      "29 ▁հայ\n",
      "\n",
      "Last 30 vocabulary entries:\n",
      "270 Բ\n",
      "271 չ\n",
      "272 ջ\n",
      "273 փ\n",
      "274 Կ\n",
      "275 Ն\n",
      "276 Տ\n",
      "277 ձ\n",
      "278 Ե\n",
      "279 Մ\n",
      "280 Գ\n",
      "281 Դ\n",
      "282 Ծ\n",
      "283 Պ\n",
      "284 օ\n",
      "285 Թ\n",
      "286 Լ\n",
      "287 Խ\n",
      "288 Շ\n",
      "289 Ռ\n",
      "290 Ս\n",
      "291 Վ\n",
      "292 Ֆ\n",
      "293 ,\n",
      "294 Ը\n",
      "295 Ի\n",
      "296 Ձ\n",
      "297 Ղ\n",
      "298 Ո\n",
      "299 Ջ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: hy_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 300\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 93 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=4406\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=66\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 93 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 93\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 332\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=230 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=20 all=909 active=843 piece=որ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=1076 active=1010 piece=ները\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=60 all=1198 active=1132 piece=իտ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=80 all=1319 active=1253 piece=արգ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1405 active=1339 piece=▁եր\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1484 active=1077 piece=շակ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=140 all=1534 active=1127 piece=▁Մ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=160 all=1562 active=1155 piece=▁ոլոր\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=180 all=1593 active=1186 piece=մբ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=200 all=1646 active=1239 piece=րագ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1661 active=1012 piece=▁հետազոտ\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: hy_bpe.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: hy_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Training Script (using existing corpus.txt)\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece BPE model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"corpus.txt\",       \n",
    "    model_prefix=\"hy_bpe\",\n",
    "    vocab_size=300,\n",
    "    model_type=\"bpe\",\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hy_bpe.model\")\n",
    "\n",
    "# Vocabulary statistics\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(\"Total vocabulary size:\", vocab_size)\n",
    "\n",
    "print(\"\\nFirst 30 vocabulary entries:\")\n",
    "for i in range(30):\n",
    "    print(i, sp.id_to_piece(i))\n",
    "\n",
    "print(\"\\nLast 30 vocabulary entries:\")\n",
    "for i in range(vocab_size - 30, vocab_size):\n",
    "    print(i, sp.id_to_piece(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
