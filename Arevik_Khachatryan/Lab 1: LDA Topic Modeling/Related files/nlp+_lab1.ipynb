{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script part 1"
      ],
      "metadata": {
        "id": "7lJC0hk1rQvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "ZZeEzlLwrNd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvGv8-5nrnyH",
        "outputId": "79305acc-c4e8-4fa7-d778-e72eae1f5b90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DW9yxk5ppjNb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logging"
      ],
      "metadata": {
        "id": "CrbQ5djdqHjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s : %(levelname)s : %(message)s\",\n",
        "    level=logging.INFO\n",
        ")"
      ],
      "metadata": {
        "id": "To70xPlKpwGg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "gsTSu-IhqLU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    random_state=42\n",
        ")\n",
        "documents = newsgroups.data[:1000]\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8dD-u3UpzNF",
        "outputId": "bfadb838-3bfc-4005-ac6f-e385f3886823"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "4_Sf2RGnqO70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text: str) -> list[str]:\n",
        "    \"\"\"Tokenise, lowercase, remove stopwords, keep words ≥ 3 chars.\"\"\"\n",
        "    tokens = []\n",
        "    for token in text.lower().split():\n",
        "        # keep only alphabetic characters\n",
        "        word = \"\".join(c for c in token if c.isalpha())\n",
        "        if len(word) >= 3 and word not in STOPWORDS:\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "print(\"Preprocessed documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2PCxzODp49n",
        "outputId": "d7eda4f0-0496-43da-be9e-8929db02668a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Dictionary & Corpus"
      ],
      "metadata": {
        "id": "d9fyEiOsqSXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "\n",
        "# Filter rare / very common words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.50)\n",
        "dictionary.compactify()\n",
        "print(f\"Dictionary size after filtering: {len(dictionary)} tokens\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "print(\"Converted to Bag-of-Words corpus\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_7eSFz8p6A1",
        "outputId": "a217c25a-ff8e-46cc-98d6-95da1d02419a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary size after filtering: 2485 tokens\n",
            "Converted to Bag-of-Words corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train LDA Model"
      ],
      "metadata": {
        "id": "tZ8GsGs6qWdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TOPICS = 10\n",
        "PASSES = 15\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(f\"Training LDA model (topics={NUM_TOPICS}, passes={PASSES})\")\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=NUM_TOPICS,\n",
        "    passes=PASSES,\n",
        "    alpha=\"auto\",\n",
        "    eta=\"auto\",\n",
        "    random_state=RANDOM_STATE,\n",
        "    per_word_topics=True\n",
        ")\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwhMhHv7p9VV",
        "outputId": "4543380e-5f74-42fb-ae73-1c77ea38eeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LDA model  (topics=10, passes=15) …\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Outputs"
      ],
      "metadata": {
        "id": "4hbf-6ZmqggA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_DIR = \"models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "lda_model.save(os.path.join(MODELS_DIR, \"lda_model\"))\n",
        "dictionary.save(os.path.join(MODELS_DIR, \"dictionary.gensim\"))\n",
        "print(f\"Model and dictionary saved to '{MODELS_DIR}/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZNHGdMjqDGj",
        "outputId": "4f03f8e9-efe2-48e2-c19f-af8aa8e23c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and dictionary saved to 'models/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Discovered Topics"
      ],
      "metadata": {
        "id": "T6jZ9--jqkHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DISCOVERED TOPICS (top 15 words each)\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "topic_labels = {}\n",
        "for topic_id in range(NUM_TOPICS):\n",
        "    top_words = lda_model.show_topic(topic_id, topn=15)\n",
        "    words_str = \", \".join(word for word, _ in top_words)\n",
        "    print(f\"\\nTopic {topic_id:02d}: {words_str}\")\n",
        "    topic_labels[topic_id] = [word for word, _ in top_words]\n",
        "\n",
        "# Save topic labels as JSON for later use\n",
        "labels_path = os.path.join(MODELS_DIR, \"topic_labels.json\")\n",
        "with open(labels_path, \"w\") as fh:\n",
        "    json.dump({str(k): v for k, v in topic_labels.items()}, fh, indent=2)\n",
        "print(f\"\\n\\nTopic labels saved to '{labels_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOsVUf2VqGYw",
        "outputId": "b0b0bad6-8b43-443e-af7f-19121b445321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DISCOVERED TOPICS (top 15 words each)\n",
            "========================================\n",
            "\n",
            "Topic 00: data, use, program, space, nasa, mission, dont, clipper, chip, applications, want, images, key, read, way\n",
            "\n",
            "Topic 01: jesus, people, god, know, argument, true, think, dont, believe, example, truth, said, matthew, point, things\n",
            "\n",
            "Topic 02: son, father, spirit, time, people, holy, years, right, space, john, earth, way, god, new, read\n",
            "\n",
            "Topic 03: armenian, people, government, genocide, xsoviet, turkish, turks, like, think, new, armenia, russian, right, muslim, got\n",
            "\n",
            "Topic 04: like, dont, files, file, use, want, people, image, know, dos, window, windows, time, bios, control\n",
            "\n",
            "Topic 05: think, windows, dont, know, want, new, speed, car, like, case, people, battery, cars, thing, going\n",
            "\n",
            "Topic 06: good, game, team, excellent, like, missing, games, year, runs, shuttle, know, dont, cover, season, fair\n",
            "\n",
            "Topic 07: people, armenians, armenian, turkish, said, killed, government, genocide, greek, gun, israel, state, came, going, left\n",
            "\n",
            "Topic 08: problem, ive, know, use, like, thanks, card, new, software, time, dont, scsi, think, memory, need\n",
            "\n",
            "Topic 09: health, use, period, power, play, medical, april, years, washington, users, san, number, like, reported, state\n",
            "\n",
            "\n",
            "Topic labels saved to 'models/topic_labels.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script part 2"
      ],
      "metadata": {
        "id": "pdWpizlpvxS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel"
      ],
      "metadata": {
        "id": "3lgnP8fyvysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paths"
      ],
      "metadata": {
        "id": "AkPOaaK6yWZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_DIR = \"models\"\n",
        "MODEL_PATH = os.path.join(MODELS_DIR, \"lda_model\")\n",
        "DICT_PATH = os.path.join(MODELS_DIR, \"dictionary.gensim\")\n",
        "LABELS_PATH = os.path.join(MODELS_DIR, \"topic_names.json\")\n",
        "TOP_N_WORDS = 20"
      ],
      "metadata": {
        "id": "F0QSMmmHv2x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model & dictionary"
      ],
      "metadata": {
        "id": "rJC4XtaCyXpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaModel.load(MODEL_PATH)\n",
        "dictionary = corpora.Dictionary.load(DICT_PATH)\n",
        "num_topics = lda_model.num_topics\n",
        "print(f\"  Loaded model with {num_topics} topics.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx-U5ctIv6Qb",
        "outputId": "9c3f86ea-fd4c-4aa8-e37a-9d2cc38dbb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded model with 10 topics.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model & dictionary"
      ],
      "metadata": {
        "id": "X-rexn9Eyd6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topic(topic_id: int, topn: int = TOP_N_WORDS) -> None:\n",
        "    \"\"\"Pretty-print a topic with word probabilities.\"\"\"\n",
        "    top_words = lda_model.show_topic(topic_id, topn=topn)\n",
        "    print(f\"\\n{'─'*55}\")\n",
        "    print(f\"  Topic {topic_id:02d}\")\n",
        "    print(f\"{'─'*55}\")\n",
        "    print(f\"  {'Word':<20} {'Probability':>12}\")\n",
        "    print(f\"  {'----':<20} {'-----------':>12}\")\n",
        "    for word, prob in top_words:\n",
        "        print(f\"  {word:<20} {prob:>12.6f}\")\n",
        "\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"       ALL DISCOVERED TOPICS  (top 20 words each)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "for tid in range(num_topics):\n",
        "    display_topic(tid)\n",
        "\n",
        "print(f\"\\n{'='*55}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCf73DbCv9hB",
        "outputId": "570ce406-eebc-4386-9f95-132390fb0243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "       ALL DISCOVERED TOPICS  (top 20 words each)\n",
            "=======================================================\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 00\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  data                     0.010870\n",
            "  use                      0.008161\n",
            "  program                  0.008092\n",
            "  space                    0.007984\n",
            "  nasa                     0.007913\n",
            "  mission                  0.006374\n",
            "  dont                     0.006235\n",
            "  clipper                  0.006172\n",
            "  chip                     0.006042\n",
            "  applications             0.005543\n",
            "  want                     0.005452\n",
            "  images                   0.005187\n",
            "  key                      0.005144\n",
            "  read                     0.004995\n",
            "  way                      0.004939\n",
            "  like                     0.004938\n",
            "  brian                    0.004656\n",
            "  application              0.004624\n",
            "  shuttle                  0.004382\n",
            "  look                     0.004267\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 01\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  jesus                    0.014104\n",
            "  people                   0.012573\n",
            "  god                      0.010744\n",
            "  know                     0.009106\n",
            "  argument                 0.008618\n",
            "  true                     0.008037\n",
            "  think                    0.007952\n",
            "  dont                     0.007843\n",
            "  believe                  0.007173\n",
            "  example                  0.006068\n",
            "  truth                    0.005680\n",
            "  said                     0.005493\n",
            "  matthew                  0.005424\n",
            "  point                    0.005281\n",
            "  things                   0.004881\n",
            "  christian                0.004783\n",
            "  good                     0.004780\n",
            "  bible                    0.004756\n",
            "  time                     0.004748\n",
            "  way                      0.004633\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 02\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  son                      0.011272\n",
            "  father                   0.010749\n",
            "  spirit                   0.008164\n",
            "  time                     0.007447\n",
            "  people                   0.006428\n",
            "  holy                     0.006351\n",
            "  years                    0.005523\n",
            "  right                    0.005110\n",
            "  space                    0.004796\n",
            "  john                     0.004728\n",
            "  earth                    0.004692\n",
            "  way                      0.004653\n",
            "  god                      0.004515\n",
            "  new                      0.004317\n",
            "  read                     0.004279\n",
            "  possible                 0.004264\n",
            "  things                   0.004001\n",
            "  information              0.003995\n",
            "  need                     0.003924\n",
            "  use                      0.003922\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 03\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  armenian                 0.011973\n",
            "  people                   0.010716\n",
            "  government               0.008404\n",
            "  genocide                 0.007163\n",
            "  xsoviet                  0.006976\n",
            "  turkish                  0.006768\n",
            "  turks                    0.006414\n",
            "  like                     0.006314\n",
            "  think                    0.006003\n",
            "  new                      0.005777\n",
            "  armenia                  0.005424\n",
            "  russian                  0.005164\n",
            "  right                    0.004701\n",
            "  muslim                   0.004673\n",
            "  got                      0.004560\n",
            "  armenians                0.004465\n",
            "  kurds                    0.004366\n",
            "  book                     0.004220\n",
            "  million                  0.004153\n",
            "  time                     0.004150\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 04\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  like                     0.011152\n",
            "  dont                     0.009917\n",
            "  files                    0.008321\n",
            "  file                     0.008123\n",
            "  use                      0.007950\n",
            "  want                     0.007321\n",
            "  people                   0.006938\n",
            "  image                    0.006823\n",
            "  know                     0.006725\n",
            "  dos                      0.006575\n",
            "  window                   0.006561\n",
            "  windows                  0.006406\n",
            "  time                     0.006031\n",
            "  bios                     0.004817\n",
            "  control                  0.004816\n",
            "  directory                0.004353\n",
            "  list                     0.004282\n",
            "  lines                    0.004282\n",
            "  security                 0.004185\n",
            "  systems                  0.004177\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 05\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  think                    0.010821\n",
            "  windows                  0.010052\n",
            "  dont                     0.008252\n",
            "  know                     0.007503\n",
            "  want                     0.007115\n",
            "  new                      0.006388\n",
            "  speed                    0.005819\n",
            "  car                      0.005747\n",
            "  like                     0.005452\n",
            "  case                     0.005241\n",
            "  people                   0.004969\n",
            "  battery                  0.004774\n",
            "  cars                     0.004656\n",
            "  thing                    0.004490\n",
            "  going                    0.004418\n",
            "  way                      0.004306\n",
            "  family                   0.004224\n",
            "  things                   0.004055\n",
            "  big                      0.004030\n",
            "  good                     0.003973\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 06\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  good                     0.022620\n",
            "  game                     0.011309\n",
            "  team                     0.010158\n",
            "  excellent                0.009490\n",
            "  like                     0.007392\n",
            "  missing                  0.007049\n",
            "  games                    0.006760\n",
            "  year                     0.006677\n",
            "  runs                     0.005462\n",
            "  shuttle                  0.005346\n",
            "  know                     0.005266\n",
            "  dont                     0.005114\n",
            "  cover                    0.004863\n",
            "  season                   0.004854\n",
            "  fair                     0.004740\n",
            "  sure                     0.004571\n",
            "  great                    0.004486\n",
            "  poster                   0.004476\n",
            "  new                      0.004439\n",
            "  thanks                   0.004409\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 07\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  people                   0.014583\n",
            "  armenians                0.012440\n",
            "  armenian                 0.009715\n",
            "  turkish                  0.008426\n",
            "  said                     0.007813\n",
            "  killed                   0.006662\n",
            "  government               0.006157\n",
            "  genocide                 0.005144\n",
            "  greek                    0.004786\n",
            "  gun                      0.004708\n",
            "  israel                   0.004545\n",
            "  state                    0.004432\n",
            "  came                     0.004355\n",
            "  going                    0.004263\n",
            "  left                     0.004060\n",
            "  city                     0.004028\n",
            "  right                    0.004012\n",
            "  father                   0.003807\n",
            "  world                    0.003545\n",
            "  war                      0.003525\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 08\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  problem                  0.011468\n",
            "  ive                      0.008581\n",
            "  know                     0.008552\n",
            "  use                      0.008367\n",
            "  like                     0.008185\n",
            "  thanks                   0.008039\n",
            "  card                     0.007858\n",
            "  new                      0.007145\n",
            "  software                 0.006314\n",
            "  time                     0.006185\n",
            "  dont                     0.006125\n",
            "  scsi                     0.006041\n",
            "  think                    0.005955\n",
            "  memory                   0.005671\n",
            "  need                     0.005646\n",
            "  email                    0.005453\n",
            "  windows                  0.005432\n",
            "  got                      0.005409\n",
            "  work                     0.005354\n",
            "  video                    0.005288\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "  Topic 09\n",
            "───────────────────────────────────────────────────────\n",
            "  Word                  Probability\n",
            "  ----                  -----------\n",
            "  health                   0.017097\n",
            "  use                      0.016492\n",
            "  period                   0.013130\n",
            "  power                    0.009703\n",
            "  play                     0.007725\n",
            "  medical                  0.006467\n",
            "  april                    0.005883\n",
            "  years                    0.005784\n",
            "  washington               0.005617\n",
            "  users                    0.005560\n",
            "  san                      0.005331\n",
            "  number                   0.005308\n",
            "  like                     0.005304\n",
            "  reported                 0.005247\n",
            "  state                    0.005124\n",
            "  university               0.005049\n",
            "  second                   0.004943\n",
            "  public                   0.004864\n",
            "  thanks                   0.004751\n",
            "  know                     0.004739\n",
            "\n",
            "=======================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive naming"
      ],
      "metadata": {
        "id": "-s-ejt83ynbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names: dict[str, str] = {}\n",
        "\n",
        "print(\"Assigning a meaningful name to each topic.\")\n",
        "\n",
        "for tid in range(num_topics):\n",
        "    default_name = f\"Topic_{tid:02d}\"\n",
        "    top_words    = [w for w, _ in lda_model.show_topic(tid, topn=5)]\n",
        "    hint         = \", \".join(top_words)\n",
        "    prompt       = f\"  Topic {tid:02d}  [{hint}]  → name: \"\n",
        "\n",
        "    user_input = input(prompt).strip()\n",
        "    chosen     = user_input if user_input else default_name\n",
        "    topic_names[str(tid)] = chosen\n",
        "    print(f\"    ✓ Topic {tid:02d} labelled as  '{chosen}'\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0StOL-wwA5y",
        "outputId": "c7720aab-6cc3-4bce-a0a0-951c5ac8e0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now assign a meaningful name to each topic.\n",
            "Press ENTER to keep the default name  (Topic_XX).\n",
            "\n",
            "  Topic 00  [data, use, program, space, nasa]  → name: Nasa/Mision\n",
            "    ✓ Topic 00 labelled as  'Nasa/Mision'\n",
            "\n",
            "  Topic 01  [jesus, people, god, know, argument]  → name: Religion\n",
            "    ✓ Topic 01 labelled as  'Religion'\n",
            "\n",
            "  Topic 02  [son, father, spirit, time, people]  → name: Time/People\n",
            "    ✓ Topic 02 labelled as  'Time/People'\n",
            "\n",
            "  Topic 03  [armenian, people, government, genocide, xsoviet]  → name: Armenian/Genocid\n",
            "    ✓ Topic 03 labelled as  'Armenian/Genocid'\n",
            "\n",
            "  Topic 04  [like, dont, files, file, use]  → name: Social Media/File System\n",
            "    ✓ Topic 04 labelled as  'Social Media/File System'\n",
            "\n",
            "  Topic 05  [think, windows, dont, know, want]  → name: Prompting wording\n",
            "    ✓ Topic 05 labelled as  'Prompting wording'\n",
            "\n",
            "  Topic 06  [good, game, team, excellent, like]  → name: Competition\n",
            "    ✓ Topic 06 labelled as  'Competition'\n",
            "\n",
            "  Topic 07  [people, armenians, armenian, turkish, said]  → name: War\n",
            "    ✓ Topic 07 labelled as  'War'\n",
            "\n",
            "  Topic 08  [problem, ive, know, use, like]  → name: Lawyer\n",
            "    ✓ Topic 08 labelled as  'Lawyer'\n",
            "\n",
            "  Topic 09  [health, use, period, power, play]  → name: Health/Medical\n",
            "    ✓ Topic 09 labelled as  'Health/Medical'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save labels"
      ],
      "metadata": {
        "id": "ikpIwL4cyoiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "with open(LABELS_PATH, \"w\") as fh:\n",
        "    json.dump(topic_names, fh, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nTopic names saved to  '{LABELS_PATH}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jti5spiwEq2",
        "outputId": "a4ed79c6-d153-4c29-b699-7a8c03fad325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic names saved to  'models/topic_names.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final summary"
      ],
      "metadata": {
        "id": "el5-PNoNyr77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"  {'ID':<6}  {'Assigned Name':<25}  Top-5 Words\")\n",
        "print(f\"  {'--':<6}  {'-------------':<25}  ----------\")\n",
        "for tid in range(num_topics):\n",
        "    name      = topic_names[str(tid)]\n",
        "    top_words = \", \".join(w for w, _ in lda_model.show_topic(tid, topn=5))\n",
        "    print(f\"  {tid:<6}  {name:<25}  {top_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzcdB1lbwJDY",
        "outputId": "cbc73ea0-40f1-4997-b497-84d465dad93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ID      Assigned Name              Top-5 Words\n",
            "  --      -------------              ----------\n",
            "  0       Nasa/Mision                data, use, program, space, nasa\n",
            "  1       Religion                   jesus, people, god, know, argument\n",
            "  2       Time/People                son, father, spirit, time, people\n",
            "  3       Armenian/Genocid           armenian, people, government, genocide, xsoviet\n",
            "  4       Social Media/File System   like, dont, files, file, use\n",
            "  5       Prompting wording          think, windows, dont, know, want\n",
            "  6       Competition                good, game, team, excellent, like\n",
            "  7       War                        people, armenians, armenian, turkish, said\n",
            "  8       Lawyer                     problem, ive, know, use, like\n",
            "  9       Health/Medical             health, use, period, power, play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script part 3"
      ],
      "metadata": {
        "id": "Kkks97dh0Riz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "FtzGuoVN0eSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS"
      ],
      "metadata": {
        "id": "I4ZUDgoK0foE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paths"
      ],
      "metadata": {
        "id": "LcFHnc9821PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_DIR = \"models\"\n",
        "MODEL_PATH = os.path.join(MODELS_DIR, \"lda_model\")\n",
        "DICT_PATH = os.path.join(MODELS_DIR, \"dictionary.gensim\")\n",
        "NAMES_PATH = os.path.join(MODELS_DIR, \"topic_names.json\")   # from Part 2\n",
        "TOP_TOPICS = 3    # how many topics to return per document\n",
        "TOP_WORDS = 5    # top words to display per topic"
      ],
      "metadata": {
        "id": "kbZvJ-Fk0iy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "VGZTrtqX22xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text: str) -> list[str]:\n",
        "    \"\"\"Tokenise, lowercase, remove stopwords, keep alphabetic words ≥ 3 chars.\"\"\"\n",
        "    tokens = []\n",
        "    for token in text.lower().split():\n",
        "        word = \"\".join(c for c in token if c.isalpha())\n",
        "        if len(word) >= 3 and word not in STOPWORDS:\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Nm9p0Z220sGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model, dictionary & topic names"
      ],
      "metadata": {
        "id": "aHlKtoz726zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model  = LdaModel.load(MODEL_PATH)\n",
        "dictionary = corpora.Dictionary.load(DICT_PATH)\n",
        "num_topics = lda_model.num_topics\n",
        "\n",
        "# Load user-defined names (fall back to default if file missing)\n",
        "if os.path.exists(NAMES_PATH):\n",
        "    with open(NAMES_PATH) as fh:\n",
        "        topic_names: dict[str, str] = json.load(fh)\n",
        "    print(f\"Topic names loaded from '{NAMES_PATH}'.\")\n",
        "else:\n",
        "    topic_names = {str(i): f\"Topic_{i:02d}\" for i in range(num_topics)}\n",
        "    print(f\"  '{NAMES_PATH}' not found – using default names.\")\n",
        "\n",
        "print(f\"Model has {num_topics} topics.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wd3pJjZ0tdE",
        "outputId": "6673d4a6-38a9-488f-e454-8736b5bac094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic names loaded from 'models/topic_names.json'.\n",
            "Model has 10 topics.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Startup: Summary of all topics"
      ],
      "metadata": {
        "id": "rkU84vxn3Euz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_topic_summary() -> None:\n",
        "    print(f\"  {'ID':<5}  {'Name':<25}  Top-5 Words\")\n",
        "    print(f\"  {'--':<5}  {'----':<25}  ----------\")\n",
        "    for tid in range(num_topics):\n",
        "        name      = topic_names.get(str(tid), f\"Topic_{tid:02d}\")\n",
        "        top_words = \", \".join(w for w, _ in lda_model.show_topic(tid, topn=5))\n",
        "        print(f\"  {tid:<5}  {name:<25}  {top_words}\")\n",
        "\n",
        "print_topic_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE_Gdzo902ub",
        "outputId": "e13bdc27-00f2-4b37-ee5d-b5360e0b4fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ID     Name                       Top-5 Words\n",
            "  --     ----                       ----------\n",
            "  0      Nasa/Mision                data, use, program, space, nasa\n",
            "  1      Religion                   jesus, people, god, know, argument\n",
            "  2      Time/People                son, father, spirit, time, people\n",
            "  3      Armenian/Genocid           armenian, people, government, genocide, xsoviet\n",
            "  4      Social Media/File System   like, dont, files, file, use\n",
            "  5      Prompting wording          think, windows, dont, know, want\n",
            "  6      Competition                good, game, team, excellent, like\n",
            "  7      War                        people, armenians, armenian, turkish, said\n",
            "  8      Lawyer                     problem, ive, know, use, like\n",
            "  9      Health/Medical             health, use, period, power, play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification function"
      ],
      "metadata": {
        "id": "i6wEb89q3F4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document(text: str, top_n: int = TOP_TOPICS) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Classify a text document.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List of dicts (sorted by probability, descending), each containing:\n",
        "        topic_id    : int\n",
        "        name        : str\n",
        "        probability : float\n",
        "        top_words   : list[str]\n",
        "    \"\"\"\n",
        "    tokens  = preprocess(text)\n",
        "    bow     = dictionary.doc2bow(tokens)\n",
        "    # get_document_topics returns sorted list by default; ensure completeness\n",
        "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
        "    topic_dist_sorted = sorted(topic_dist, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    results = []\n",
        "    for tid, prob in topic_dist_sorted[:top_n]:\n",
        "        results.append({\n",
        "            \"topic_id\"   : tid,\n",
        "            \"name\"       : topic_names.get(str(tid), f\"Topic_{tid:02d}\"),\n",
        "            \"probability\": float(prob),\n",
        "            \"top_words\"  : [w for w, _ in lda_model.show_topic(tid, topn=TOP_WORDS)],\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "hK4YpfTH0_lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display helper"
      ],
      "metadata": {
        "id": "viEzxlTI3KzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_classification(doc_num: int, text: str) -> None:\n",
        "    PREVIEW_LEN = 200\n",
        "    preview = text.strip().replace(\"\\n\", \" \")\n",
        "    preview = preview[:PREVIEW_LEN] + (\"…\" if len(preview) > PREVIEW_LEN else \"\")\n",
        "\n",
        "    print(\"─\" * 62)\n",
        "    print(f\"  Document {doc_num}\")\n",
        "    print(\"─\" * 62)\n",
        "    print(f\"  Preview : {preview}\\n\")\n",
        "\n",
        "    results = classify_document(text)\n",
        "\n",
        "    print(f\"  {'Rank':<5}  {'Topic Name':<25}  {'Probability':>11}  Top-5 Words\")\n",
        "    print(f\"  {'----':<5}  {'----------':<25}  {'-----------':>11}  ----------\")\n",
        "    for rank, r in enumerate(results, start=1):\n",
        "        words_str = \", \".join(r[\"top_words\"])\n",
        "        print(f\"  {rank:<5}  {r['name']:<25}  {r['probability']:>11.4f}  {words_str}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "jNGwA5B-1GLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample documents"
      ],
      "metadata": {
        "id": "vV-cc_eD3Osz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_DOCS = [\n",
        "    # 1 – Gaming / Technology\n",
        "    (\n",
        "        \"Gaming Technology\",\n",
        "        \"The new graphics card delivers amazing performance for gaming. The GPU can \"\n",
        "        \"handle 4K resolution easily with ray tracing enabled. Gamers will love the improved \"\n",
        "        \"frame rates.\"\n",
        "    ),\n",
        "\n",
        "    # 2 – Science / Space\n",
        "    (\n",
        "        \"Space Science\",\n",
        "        \"Scientists discovered a new exoplanet orbiting a distant star in the habitable zone. \"\n",
        "        \"The research team published their findings in Nature journal. This discovery could \"\n",
        "        \"provide insights into planetary formation.\"\n",
        "    ),\n",
        "\n",
        "    # 3 – Sports\n",
        "    (\n",
        "        \"Sports\",\n",
        "        \"The basketball team won the championship after an incredible final game. The \"\n",
        "        \"players celebrated with fans in the stadium. It was the team's first title in twenty \"\n",
        "        \"years.\"\n",
        "    ),\n",
        "\n",
        "    # 4 – Politics / Government\n",
        "    (\n",
        "        \"Politics\",\n",
        "        \"Congress passed a new bill regarding healthcare reform. The president is expected \"\n",
        "        \"to sign the legislation next week. The policy will affect millions of citizens across the \"\n",
        "        \"country.\"\n",
        "    ),\n",
        "\n",
        "    # 5 – Food / Cooking\n",
        "    (\n",
        "        \"Food & Cooking\",\n",
        "        \"I love cooking Italian food at home. Pasta carbonara and margherita pizza are my \"\n",
        "        \"favorite dishes to make. Fresh ingredients make all the difference in authentic \"\n",
        "        \"recipes.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "Kl3MzW521MbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run example classifications"
      ],
      "metadata": {
        "id": "qcn5nrFU3SFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (label, doc) in enumerate(SAMPLE_DOCS, start=1):\n",
        "    print(f\"  Sample Topic: {label}\")\n",
        "    display_classification(idx, doc)\n",
        "\n",
        "print(\"Classification complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT5nqLks1POU",
        "outputId": "29131794-7fa5-4104-ed5c-0852c4bbde9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sample Topic: Gaming Technology\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Document 1\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Preview : The new graphics card delivers amazing performance for gaming. The GPU can handle 4K resolution easily with ray tracing enabled. Gamers will love the improved frame rates.\n",
            "\n",
            "  Rank   Topic Name                 Probability  Top-5 Words\n",
            "  ----   ----------                 -----------  ----------\n",
            "  1      Lawyer                          0.6343  problem, ive, know, use, like\n",
            "  2      Social Media/File System        0.3284  like, dont, files, file, use\n",
            "  3      Religion                        0.0059  jesus, people, god, know, argument\n",
            "\n",
            "  Sample Topic: Space Science\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Document 2\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Preview : Scientists discovered a new exoplanet orbiting a distant star in the habitable zone. The research team published their findings in Nature journal. This discovery could provide insights into planetary …\n",
            "\n",
            "  Rank   Topic Name                 Probability  Top-5 Words\n",
            "  ----   ----------                 -----------  ----------\n",
            "  1      Time/People                     0.6509  son, father, spirit, time, people\n",
            "  2      Competition                     0.1756  good, game, team, excellent, like\n",
            "  3      Armenian/Genocid                0.1293  armenian, people, government, genocide, xsoviet\n",
            "\n",
            "  Sample Topic: Sports\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Document 3\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Preview : The basketball team won the championship after an incredible final game. The players celebrated with fans in the stadium. It was the team's first title in twenty years.\n",
            "\n",
            "  Rank   Topic Name                 Probability  Top-5 Words\n",
            "  ----   ----------                 -----------  ----------\n",
            "  1      Competition                     0.7223  good, game, team, excellent, like\n",
            "  2      Armenian/Genocid                0.1240  armenian, people, government, genocide, xsoviet\n",
            "  3      Prompting wording               0.1141  think, windows, dont, know, want\n",
            "\n",
            "  Sample Topic: Politics\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Document 4\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Preview : Congress passed a new bill regarding healthcare reform. The president is expected to sign the legislation next week. The policy will affect millions of citizens across the country.\n",
            "\n",
            "  Rank   Topic Name                 Probability  Top-5 Words\n",
            "  ----   ----------                 -----------  ----------\n",
            "  1      Armenian/Genocid                0.6723  armenian, people, government, genocide, xsoviet\n",
            "  2      Prompting wording               0.2935  think, windows, dont, know, want\n",
            "  3      Lawyer                          0.0063  problem, ive, know, use, like\n",
            "\n",
            "  Sample Topic: Food & Cooking\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Document 5\n",
            "──────────────────────────────────────────────────────────────\n",
            "  Preview : I love cooking Italian food at home. Pasta carbonara and margherita pizza are my favorite dishes to make. Fresh ingredients make all the difference in authentic recipes.\n",
            "\n",
            "  Rank   Topic Name                 Probability  Top-5 Words\n",
            "  ----   ----------                 -----------  ----------\n",
            "  1      Armenian/Genocid                0.5398  armenian, people, government, genocide, xsoviet\n",
            "  2      Religion                        0.3850  jesus, people, god, know, argument\n",
            "  3      Lawyer                          0.0143  problem, ive, know, use, like\n",
            "\n",
            "Classification complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Կարծում եմ 1000-ը քիչ է եղել, չափը մեծացնելը կլավացնի արդյունքները"
      ],
      "metadata": {
        "id": "wONonS0u7MtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}