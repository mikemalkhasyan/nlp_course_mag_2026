{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b486224b-bb40-40e5-a498-8bfaba98d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model...\n",
      "\n",
      "Discovered Topics (Top 15 words):\n",
      "Topic 0: 0.108*\"max\" + 0.032*\"145\" + 0.013*\"year\" + 0.008*\"insurance\" + 0.007*\"like\" + 0.007*\"state\" + 0.006*\"edu\" + 0.005*\"university\" + 0.005*\"think\" + 0.004*\"great\" + 0.004*\"look\" + 0.004*\"want\" + 0.004*\"100\" + 0.004*\"heard\" + 0.004*\"good\"\n",
      "Topic 1: 0.014*\"space\" + 0.011*\"like\" + 0.008*\"use\" + 0.007*\"scsi\" + 0.007*\"shuttle\" + 0.006*\"nasa\" + 0.006*\"bit\" + 0.006*\"know\" + 0.006*\"image\" + 0.005*\"want\" + 0.005*\"files\" + 0.005*\"chip\" + 0.005*\"orbit\" + 0.005*\"mission\" + 0.005*\"power\"\n",
      "Topic 2: 0.016*\"god\" + 0.012*\"know\" + 0.010*\"true\" + 0.010*\"argument\" + 0.009*\"believe\" + 0.008*\"truth\" + 0.008*\"example\" + 0.008*\"way\" + 0.008*\"think\" + 0.007*\"people\" + 0.007*\"bible\" + 0.007*\"spirit\" + 0.007*\"good\" + 0.006*\"son\" + 0.006*\"father\"\n",
      "Topic 3: 0.010*\"said\" + 0.009*\"time\" + 0.008*\"like\" + 0.008*\"people\" + 0.006*\"gun\" + 0.006*\"think\" + 0.005*\"way\" + 0.005*\"right\" + 0.005*\"know\" + 0.005*\"problem\" + 0.005*\"got\" + 0.005*\"israel\" + 0.004*\"away\" + 0.004*\"course\" + 0.004*\"going\"\n",
      "Topic 4: 0.021*\"people\" + 0.017*\"armenian\" + 0.014*\"armenians\" + 0.012*\"turkish\" + 0.009*\"genocide\" + 0.008*\"government\" + 0.007*\"period\" + 0.007*\"soviet\" + 0.006*\"killed\" + 0.005*\"power\" + 0.005*\"war\" + 0.005*\"play\" + 0.005*\"russian\" + 0.005*\"turks\" + 0.004*\"world\"\n",
      "Topic 5: 0.013*\"health\" + 0.011*\"use\" + 0.009*\"1993\" + 0.008*\"greek\" + 0.006*\"program\" + 0.006*\"time\" + 0.006*\"years\" + 0.006*\"number\" + 0.006*\"medical\" + 0.005*\"radius\" + 0.005*\"problem\" + 0.005*\"national\" + 0.005*\"security\" + 0.005*\"think\" + 0.005*\"public\"\n",
      "Topic 6: 0.007*\"time\" + 0.007*\"know\" + 0.006*\"better\" + 0.006*\"years\" + 0.006*\"new\" + 0.006*\"year\" + 0.005*\"000\" + 0.005*\"runs\" + 0.005*\"use\" + 0.005*\"box\" + 0.004*\"high\" + 0.004*\"team\" + 0.004*\"going\" + 0.004*\"like\" + 0.004*\"thanks\"\n",
      "Topic 7: 0.011*\"edu\" + 0.009*\"government\" + 0.008*\"think\" + 0.008*\"church\" + 0.005*\"new\" + 0.005*\"people\" + 0.004*\"believe\" + 0.004*\"point\" + 0.004*\"like\" + 0.004*\"asking\" + 0.004*\"good\" + 0.004*\"yes\" + 0.004*\"end\" + 0.004*\"objectively\" + 0.004*\"cause\"\n",
      "Topic 8: 0.041*\"jesus\" + 0.017*\"matthew\" + 0.012*\"people\" + 0.009*\"john\" + 0.007*\"god\" + 0.007*\"david\" + 0.007*\"new\" + 0.006*\"king\" + 0.006*\"says\" + 0.006*\"day\" + 0.006*\"lord\" + 0.005*\"things\" + 0.005*\"time\" + 0.005*\"said\" + 0.005*\"good\"\n",
      "Topic 9: 0.017*\"windows\" + 0.013*\"good\" + 0.011*\"thanks\" + 0.011*\"use\" + 0.008*\"mail\" + 0.008*\"like\" + 0.007*\"know\" + 0.007*\"need\" + 0.007*\"help\" + 0.007*\"card\" + 0.007*\"excellent\" + 0.007*\"software\" + 0.006*\"memory\" + 0.006*\"problem\" + 0.006*\"bit\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import re\n",
    "\n",
    "# 1. Load Dataset \n",
    "data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "documents = data.data[:1000] \n",
    "\n",
    "# 2. Preprocessing \n",
    "def preprocess(text):\n",
    "    # Tokenize, lowercase, and filter by length \n",
    "    tokens = [token.lower() for token in re.findall(r'\\b\\w+\\b', text) if len(token) >= 3]\n",
    "    # Remove stopwords \n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# 3. Dictionary and Corpus \n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "# Filter rare and common words \n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# 4. Train LDA Model \n",
    "print(\"Training LDA model...\")\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    "    passes=15,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# 5. Saving \n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "lda_model.save(\"models/lda_model.model\")\n",
    "dictionary.save(\"models/lda_dict.dict\")\n",
    "\n",
    "# 6. Print Topics\n",
    "print(\"\\nDiscovered Topics (Top 15 words):\")\n",
    "for idx, topic in lda_model.print_topics(num_words=15):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
