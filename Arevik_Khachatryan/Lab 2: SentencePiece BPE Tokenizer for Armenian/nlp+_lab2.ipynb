{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece BPE Tokenizer for Armenian"
      ],
      "metadata": {
        "id": "LiHZk6RiYnZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Training Script"
      ],
      "metadata": {
        "id": "LB0aQb7tZKaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the library"
      ],
      "metadata": {
        "id": "PQZdgaZfd_hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "PPjK705NYvMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the BPE model"
      ],
      "metadata": {
        "id": "x-i74pAsZG5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    input='corpus.txt',\n",
        "    model_prefix='hy_bpe', # output: hy_bpe.model + hy_bpe.vocab\n",
        "    vocab_size=300, # vocabulary size\n",
        "    model_type='bpe', # Byte Pair Encoding\n",
        "    character_coverage=1.0, # include ALL Armenian Unicode characters\n",
        "    pad_id=0, # <pad>  Padding token\n",
        "    unk_id=1, # <unk>  Unknown token\n",
        "    bos_id=2, # <s>    Beginning Of Sentence token\n",
        "    eos_id=3, # </s>   End Of Sentence token\n",
        ")\n",
        "\n",
        "print(\"Generated files: hy_bpe.model,  hy_bpe.vocab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h00xSqOFYybz",
        "outputId": "bdeaf2bb-0b5b-4097-acc5-5f48f458080d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated files: hy_bpe.model,  hy_bpe.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load the trained model and inspect the vocabulary"
      ],
      "metadata": {
        "id": "TSlY4ybxZSM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('hy_bpe.model')\n",
        "\n",
        "total_vocab = sp.get_piece_size()\n",
        "print(f\"Total tokens in vocabulary: {total_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz95xxWzY0bb",
        "outputId": "559fb1ad-2511-4d52-9f7e-f5d07826ba64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in vocabulary: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first 30 vocabulary entries"
      ],
      "metadata": {
        "id": "Qm4iVWLgZYZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 30 vocabulary entries\")\n",
        "print(f\"{'ID':>5}  {'Token'}\\n\")\n",
        "for i in range(30):\n",
        "    piece = sp.id_to_piece(i)\n",
        "    print(f\"{i:>5}  {piece}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3dlbkPXY20m",
        "outputId": "b97212b3-e182-46e2-cbc9-9e500b0ee036"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 30 vocabulary entries\n",
            "   ID  Token\n",
            "\n",
            "    0  <pad>\n",
            "    1  <unk>\n",
            "    2  <s>\n",
            "    3  </s>\n",
            "    4  ’∏÷Ç\n",
            "    5  ’°’∂\n",
            "    6  ’°’µ\n",
            "    7  ’•÷Ä\n",
            "    8  ’°÷Ä\n",
            "    9  ’∏÷Ç’∂\n",
            "   10  ‚ñÅ’∞\n",
            "   11  ’∏÷Ç’¥\n",
            "   12  ’°’Ø\n",
            "   13  ’∏÷Ç’©\n",
            "   14  ‚ñÅ’ß\n",
            "   15  ’∏÷Ç’©’µ\n",
            "   16  ’•’∂\n",
            "   17  ’∏÷Ç’©’µ’∏÷Ç’∂\n",
            "   18  ‚ñÅ’Ä\n",
            "   19  ’∂’•÷Ä\n",
            "   20  ’°’Ω\n",
            "   21  ‚ñÅ’Ä’°’µ\n",
            "   22  ‚ñÅ’Ø\n",
            "   23  ’∏÷Ä\n",
            "   24  ’°’¥\n",
            "   25  ’°’Ø’°’∂\n",
            "   26  ’•÷Ç\n",
            "   27  ’°’ø\n",
            "   28  ‚ñÅ’•’∂\n",
            "   29  ‚ñÅ’¥\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print last 30 vocabulary entries"
      ],
      "metadata": {
        "id": "CdgC-eMJZfD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Last 30 vocabulary entries\")\n",
        "print(f\"{'ID':>5}  {'Token'}\\n\")\n",
        "\n",
        "for i in range(total_vocab - 30, total_vocab):\n",
        "    piece = sp.id_to_piece(i)\n",
        "    print(f\"{i:>5}  {piece}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW4fM01bY6IV",
        "outputId": "f0032977-704f-4a20-f18d-5b08cfaa2c37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 30 vocabulary entries\n",
            "   ID  Token\n",
            "\n",
            "  270  ‘≤\n",
            "  271  ’π\n",
            "  272  ’ª\n",
            "  273  ÷É\n",
            "  274  ‘ø\n",
            "  275  ’Ü\n",
            "  276  ’è\n",
            "  277  ’±\n",
            "  278  ‘µ\n",
            "  279  ’Ñ\n",
            "  280  ‘≥\n",
            "  281  ‘¥\n",
            "  282  ‘æ\n",
            "  283  ’ä\n",
            "  284  ÷Ö\n",
            "  285  ‘π\n",
            "  286  ‘º\n",
            "  287  ‘Ω\n",
            "  288  ’á\n",
            "  289  ’å\n",
            "  290  ’ç\n",
            "  291  ’é\n",
            "  292  ’ñ\n",
            "  293  ,\n",
            "  294  ‘∏\n",
            "  295  ‘ª\n",
            "  296  ’Å\n",
            "  297  ’Ç\n",
            "  298  ’à\n",
            "  299  ’ã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations guide"
      ],
      "metadata": {
        "id": "X218RVbDbAqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "Observation Guide\n",
        "‚Ä¢ IDs 0-3: special tokens: <pad>, <unk>, <s>, </s>\n",
        "‚Ä¢ Short entries: single Armenian characters (base alphabet coverage)\n",
        "‚Ä¢ Medium entries: morpheme fragments / common suffixes (BPE merges)\n",
        "‚Ä¢ Long entries: frequent full words found in the corpus\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN4hqxMKa7LO",
        "outputId": "e35f33d1-4784-4c57-f5d1-d11b138015a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation Guide\n",
            "‚Ä¢ IDs 0-3: special tokens: <pad>, <unk>, <s>, </s>\n",
            "‚Ä¢ Short entries: single Armenian characters (base alphabet coverage)\n",
            "‚Ä¢ Medium entries: morpheme fragments / common suffixes (BPE merges)\n",
            "‚Ä¢ Long entries: frequent full words found in the corpus\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Encoding and Decoding Script"
      ],
      "metadata": {
        "id": "YUN5XgdGd4u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the library"
      ],
      "metadata": {
        "id": "EAv97k3jeGDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "KGzK_ghzdtOU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the trained model"
      ],
      "metadata": {
        "id": "b2Qs-pmtd88H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('hy_bpe.model')\n",
        "print(\"Model loaded: hy_bpe.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLiEGzX_dvsK",
        "outputId": "138b1d2b-eddc-48d7-dd28-a48945aa041e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: hy_bpe.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test sentences"
      ],
      "metadata": {
        "id": "reRdkwz0eNd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = {\n",
        "    \"S1\": \"’Ä’°’µ’°’Ω’ø’°’∂’∂ ’∏÷Ç’∂’´ ’∞’°÷Ä’∏÷Ç’Ω’ø ’∫’°’ø’¥’∏÷Ç’©’µ’∏÷Ç’∂÷â\",\n",
        "    \"S2\": \"‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’°÷Ä’°’£ ’¶’°÷Ä’£’°’∂’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"S3\": \"‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä÷á’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"S4\": \"‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä’•÷Ç’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "_lfn_TTGdx2C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode, decode, and verify each sentence"
      ],
      "metadata": {
        "id": "3mtPX3WVeRMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, sentence in sentences.items():\n",
        "    print(f\"{label}: {sentence}\")\n",
        "\n",
        "    # Encode to token pieces (subword strings)\n",
        "    pieces = sp.encode(sentence, out_type=str)\n",
        "    print(f\"\\nToken pieces: {pieces}\")\n",
        "\n",
        "    # Encode to token IDs (integers)\n",
        "    ids = sp.encode(sentence, out_type=int)\n",
        "    print(f\"Token IDs: {ids}\")\n",
        "\n",
        "    # Decode IDs back to text\n",
        "    decoded = sp.decode(ids)\n",
        "    print(f\"Decoded text: {decoded}\")\n",
        "\n",
        "    # Verify round-trip\n",
        "    match = (decoded == sentence)\n",
        "    print(f\"Match original: {match}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In9VTDt5dcwH",
        "outputId": "d111d1d2-60d7-49ce-cb31-35fc1cfcdd71"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1: ’Ä’°’µ’°’Ω’ø’°’∂’∂ ’∏÷Ç’∂’´ ’∞’°÷Ä’∏÷Ç’Ω’ø ’∫’°’ø’¥’∏÷Ç’©’µ’∏÷Ç’∂÷â\n",
            "\n",
            "Token pieces: ['‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂', '’∂', '‚ñÅ’∏÷Ç’∂’´', '‚ñÅ’∞’°÷Ä’∏÷Ç’Ω’ø', '‚ñÅ’∫', '’°’ø', '’¥', '’∏÷Ç’©’µ’∏÷Ç’∂', '÷â']\n",
            "Token IDs: [36, 236, 61, 222, 96, 27, 242, 17, 246]\n",
            "Decoded text: ’Ä’°’µ’°’Ω’ø’°’∂’∂ ’∏÷Ç’∂’´ ’∞’°÷Ä’∏÷Ç’Ω’ø ’∫’°’ø’¥’∏÷Ç’©’µ’∏÷Ç’∂÷â\n",
            "Match original: True\n",
            "\n",
            "S2: ‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’°÷Ä’°’£ ’¶’°÷Ä’£’°’∂’∏÷Ç’¥ ’ß÷â\n",
            "\n",
            "Token pieces: ['‚ñÅ‘±÷Ä', '’∞', '’•’Ω’ø', '’°’Ø’°’∂', '‚ñÅ’¢', '’°’∂', '’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’®', '‚ñÅ’°÷Ä’°’£', '‚ñÅ’¶’°÷Ä’£', '’°’∂’∏÷Ç’¥', '‚ñÅ’ß', '÷â']\n",
            "Token IDs: [150, 247, 99, 25, 57, 5, 230, 162, 133, 159, 14, 246]\n",
            "Decoded text: ‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’°÷Ä’°’£ ’¶’°÷Ä’£’°’∂’∏÷Ç’¥ ’ß÷â\n",
            "Match original: True\n",
            "\n",
            "S3: ‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä÷á’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\n",
            "\n",
            "Token pieces: ['‚ñÅ‘æ', '÷Ä’°’£', '÷Ä’°', '’æ', '’∏÷Ä', '’∏÷Ç’¥’®', '‚ñÅ’Ø’°÷Ä’•÷Ç’∏÷Ä', '‚ñÅ’∞', '’¥', '’ø', '’∏÷Ç’©’µ’∏÷Ç’∂', '‚ñÅ’ß', '‚ñÅ’°’∫’°’£’°’µ’´', '‚ñÅ’∞’°’¥’°÷Ä', '÷â']\n",
            "Token IDs: [189, 203, 74, 252, 23, 209, 41, 10, 242, 244, 17, 14, 221, 166, 246]\n",
            "Decoded text: ‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä’•÷Ç’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\n",
            "Match original: False\n",
            "\n",
            "S4: ‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä’•÷Ç’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\n",
            "\n",
            "Token pieces: ['‚ñÅ‘æ', '÷Ä’°’£', '÷Ä’°', '’æ', '’∏÷Ä', '’∏÷Ç’¥’®', '‚ñÅ’Ø’°÷Ä’•÷Ç’∏÷Ä', '‚ñÅ’∞', '’¥', '’ø', '’∏÷Ç’©’µ’∏÷Ç’∂', '‚ñÅ’ß', '‚ñÅ’°’∫’°’£’°’µ’´', '‚ñÅ’∞’°’¥’°÷Ä', '÷â']\n",
            "Token IDs: [189, 203, 74, 252, 23, 209, 41, 10, 242, 244, 17, 14, 221, 166, 246]\n",
            "Decoded text: ‘æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’® ’Ø’°÷Ä’•÷Ç’∏÷Ä ’∞’¥’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\n",
            "Match original: True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ’Ü’Ø’°’ø’•÷Å’´ ’∏÷Ä \"÷á\" ÷á \"’•÷Ç\" ’ø’°÷Ä’¢’•÷Ä’∏÷Ç’©’µ’°’∂ ’∫’°’ø’≥’°’º’∏’æ 3-÷Ä’§ ÷Ö÷Ä’´’∂’°’Ø’® False ’ß ’Ω’ø’°’∂’∏÷Ç’¥, ’æ’°÷Ä’Ø’°’Æ’® ’Ω’ø’∏÷Ç’£’•÷Å’´ ’°’æ’•’¨’°÷Å’∂’•’¨’∏’æ 4-÷Ä’§ ÷Ö÷Ä’´’∂’°’Ø’®"
      ],
      "metadata": {
        "id": "0kN9VLRdeyBT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Part 3: Vocabulary Analysis Script"
      ],
      "metadata": {
        "id": "xuN6BfbMk6Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "DkJpL5ywgQFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import collections"
      ],
      "metadata": {
        "id": "N6QKQTYof5UR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and corpus"
      ],
      "metadata": {
        "id": "D95mULSigXyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('hy_bpe.model')\n",
        "print(\"Model loaded: hy_bpe.model\")\n",
        "\n",
        "with open('corpus.txt', encoding='utf-8') as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "total_vocab = sp.get_piece_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzMc6pi_f8q9",
        "outputId": "4f1ab7c4-ca86-4d3a-91da-aee839e37ca1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: hy_bpe.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorise vocabulary entries by length"
      ],
      "metadata": {
        "id": "puxUdeMRgbXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_chars  = [] # length == 1\n",
        "subword_frags = [] # length 2‚Äì4\n",
        "full_words    = [] # length >= 5\n",
        "\n",
        "# Special token IDs to skip (pad, unk, bos, eos)\n",
        "SPECIAL_IDS = {0, 1, 2, 3}\n",
        "\n",
        "for i in range(total_vocab):\n",
        "    if i in SPECIAL_IDS:\n",
        "        continue\n",
        "    piece = sp.id_to_piece(i)\n",
        "    # Strip the SentencePiece space prefix ‚ñÅ for length measurement\n",
        "    clean = piece.lstrip('‚ñÅ')\n",
        "    length = len(clean)\n",
        "\n",
        "    if length == 1:\n",
        "        single_chars.append(piece)\n",
        "    elif 2 <= length <= 4:\n",
        "        subword_frags.append(piece)\n",
        "    else:\n",
        "        full_words.append(piece)\n",
        "\n",
        "print(\"Vocabulary Structure\")\n",
        "print(f\"\\nSingle characters (length = 1): {len(single_chars)}\")\n",
        "print(f\"Subword fragments (length 2‚Äì4): {len(subword_frags)}\")\n",
        "print(f\"Full words (length 5+): {len(full_words)}\")\n",
        "print(f\"Special tokens (pad/unk/bos/eos): {len(SPECIAL_IDS)}\")\n",
        "print(f\"\\nTotal vocabulary: {total_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vInonMrCf_Og",
        "outputId": "dfcc80af-bc8a-473c-a2b8-d1fcb862d133"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Structure\n",
            "\n",
            "Single characters (length = 1): 100\n",
            "Subword fragments (length 2‚Äì4): 155\n",
            "Full words (length 5+): 41\n",
            "Special tokens (pad/unk/bos/eos): 4\n",
            "\n",
            "Total vocabulary: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 most frequent token pieces across the entire corpus"
      ],
      "metadata": {
        "id": "z6jEAIJigfUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 10 Most Frequent Token Pieces (full corpus)\")\n",
        "\n",
        "token_counts: collections.Counter = collections.Counter()\n",
        "\n",
        "for line in corpus_text.splitlines():\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "        pieces = sp.encode(line, out_type=str)\n",
        "        token_counts.update(pieces)\n",
        "\n",
        "print(f\"  {'Rank':>4}  {'Token':<3}  {'Count':>4}\\n\")\n",
        "\n",
        "for rank, (piece, count) in enumerate(token_counts.most_common(10), start=1):\n",
        "    print(f\"  {rank:>4}  {piece:<3}  {count:>4}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCIt1e0wgCGB",
        "outputId": "5b3a9c8c-0c10-4171-bc26-6039cc114bc4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Token Pieces (full corpus)\n",
            "  Rank  Token  Count\n",
            "\n",
            "     1  ÷â      93\n",
            "     2  ‚ñÅ’ß     53\n",
            "     3  ‚ñÅ      41\n",
            "     4  ’∂      35\n",
            "     5  ’°’∂     33\n",
            "     6  ’´      33\n",
            "     7  ‚ñÅ’•’∂    27\n",
            "     8  ’®      25\n",
            "     9  ÷Ä      22\n",
            "    10  ’∏÷Ç’¥    20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample tokens from each category (for illustration)"
      ],
      "metadata": {
        "id": "F6otcjqigk8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample Single Characters:\")\n",
        "print(\"  \", single_chars[:20])\n",
        "\n",
        "print(\"\\nSample Subword Fragments:\")\n",
        "print(\"  \", subword_frags[:20])\n",
        "\n",
        "print(\"\\nSample Full Words:\")\n",
        "print(\"  \", full_words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPn02C59gEHs",
        "outputId": "08c03ddb-bfa4-4b6a-f579-c7cff7cf2307"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Single Characters:\n",
            "   ['‚ñÅ’∞', '‚ñÅ’ß', '‚ñÅ’Ä', '‚ñÅ’Ø', '‚ñÅ’¥', '‚ñÅ’°', '‚ñÅ’¨', '‚ñÅ’∑', '‚ñÅ’£', '‚ñÅ’§', '‚ñÅ‘±', '‚ñÅ’¢', '‚ñÅ’™', '‚ñÅ‘≤', '‚ñÅ’ø', '‚ñÅ‘ø', '‚ñÅ’Ü', '‚ñÅ’è', '‚ñÅ’¶', '‚ñÅ’∫']\n",
            "\n",
            "Sample Subword Fragments:\n",
            "   ['’∏÷Ç', '’°’∂', '’°’µ', '’•÷Ä', '’°÷Ä', '’∏÷Ç’∂', '’∏÷Ç’¥', '’°’Ø', '’∏÷Ç’©', '’∏÷Ç’©’µ', '’•’∂', '’∂’•÷Ä', '’°’Ω', '‚ñÅ’Ä’°’µ', '’∏÷Ä', '’°’¥', '’°’Ø’°’∂', '’•÷Ç', '’°’ø', '‚ñÅ’•’∂']\n",
            "\n",
            "Sample Full Words:\n",
            "   ['’∏÷Ç’©’µ’∏÷Ç’∂', '’∏÷Ç’©’µ’∏÷Ç’∂’®', '’°’Ω’ø’°’∂', '‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂', '‚ñÅ’Ø’°÷Ä’•÷Ç’∏÷Ä', '‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂’∏÷Ç’¥', '’Ø’°’Ø’°’∂', '’∏÷Ç’©’µ’°’∂', '‚ñÅ’°’æ’°’∂’§', '‚ñÅ’∏÷Ç’∂’•’∂', '‚ñÅ’°’∑’≠’°÷Ä’∞', '’∏÷Ç’©’µ’∏÷Ç’∂’∂', '‚ñÅ’™’∏’≤’∏’æ', '‚ñÅ’°’æ’°’∂’§’°’Ø’°’∂', '’°’∂’∏÷Ç’¥', '’´÷Ä’∏÷Ç’¥', '’°’Ø’°’Ø’´÷Å', '‚ñÅ’™’°’¥’°’∂', '‚ñÅ’∞’°’¥’°÷Ä', '‚ñÅ’¨’•’¶’∏÷Ç’∂']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "In Armenian \"’∏÷Ç\" and \"’•÷Ç\" are a single letter, but there are two unicode characters, and they counted as subwords. Frequently\n",
        "occurring suffixes and grammatical endings, such as \"’∏÷Ç’¥\" merged early by BPE. Common\n",
        "high-frequency words like \"’∞’°’µ\" survive as full tokens because\n",
        "they appear repeatedly across many sentences without being broken further. Longer,\n",
        "less-frequent words like\n",
        "\"’°÷Ä’∞’•’Ω’ø’°’Ø’°’∂\" or \"’Æ÷Ä’°’£÷Ä’°’æ’∏÷Ä’∏÷Ç’¥’®\" are split into multiple subword pieces,\n",
        "showing that BPE trades off between compression and coverage. Overall, the 300-token\n",
        "vocabulary manages to represent all Armenian Unicode characters (character_coverage=1.0)\n",
        "while still learning meaningful morphological units from even this small 93-sentence corpus.\n"
      ],
      "metadata": {
        "id": "-HKvxlcHg2xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Task: Large-Scale Training\n",
        "SentencePiece BPE Tokenizer ‚Äî CC-100 Armenian Dataset"
      ],
      "metadata": {
        "id": "tQMBk8czoHE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "G4bP18vupBJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import re\n",
        "import unicodedata\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from huggingface_hub import hf_hub_download\n",
        "import lzma\n",
        "import os"
      ],
      "metadata": {
        "id": "6ibqvo8jocgx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading CC-100 Armenian dataset"
      ],
      "metadata": {
        "id": "c7Mp0x2dvmWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAW_FILE_XZ  = 'hy.txt.xz' # compressed download\n",
        "RAW_FILE_TXT = 'hy.txt' # decompressed plain text\n",
        "\n",
        "if not os.path.exists(RAW_FILE_TXT):\n",
        "    if not os.path.exists(RAW_FILE_XZ):\n",
        "        os.system(\n",
        "            \"wget -q --show-progress \"\n",
        "            \"https://data.statmt.org/cc-100/hy.txt.xz \"\n",
        "            f\"-O {RAW_FILE_XZ}\"\n",
        "        )\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    with lzma.open(RAW_FILE_XZ, 'rt', encoding='utf-8') as f_in, \\\n",
        "         open(RAW_FILE_TXT, 'w', encoding='utf-8') as f_out:\n",
        "        for line in f_in:\n",
        "            f_out.write(line)\n",
        "    print(f\"Decompressed to: {RAW_FILE_TXT}\")\n",
        "else:\n",
        "    print(f\"Found cached file: {RAW_FILE_TXT} (skipping download)\")\n",
        "\n",
        "# Count total lines for information\n",
        "with open(RAW_FILE_TXT, encoding='utf-8') as f:\n",
        "    total_lines = sum(1 for line in f if line.strip())\n",
        "\n",
        "print(f\"Total available sentences in CC-100 Armenian: {total_lines:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tML2rinjutGh",
        "outputId": "a81313cb-9af8-4a38-fa33-6a54bb5c1a36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found cached file: hy.txt (skipping download)\n",
            "Total available sentences in CC-100 Armenian: 307,594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract 50,000 Sentences and Save to File"
      ],
      "metadata": {
        "id": "Gby1ENjUwCtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = 50_000\n",
        "LARGE_CORPUS = 'corpus_large.txt'\n",
        "\n",
        "sentences_large = []\n",
        "\n",
        "with open(RAW_FILE_TXT, encoding='utf-8') as f:\n",
        "    for raw in f:\n",
        "        if len(sentences_large) >= TARGET:\n",
        "            break\n",
        "        line = raw.strip()\n",
        "        if line:\n",
        "            sentences_large.append(line)\n",
        "\n",
        "print(f\"Sentences collected : {len(sentences_large):,}\")\n",
        "\n",
        "# Save to file ‚Äî SentencePiece reads from a file path\n",
        "with open(LARGE_CORPUS, 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(sentences_large))\n",
        "\n",
        "print(f\"\\nSaved to: {LARGE_CORPUS}\")\n",
        "\n",
        "# Quick preview\n",
        "print(\"\\nSample sentences from CC-100\")\n",
        "for i, line in enumerate(sentences_large[:5]):\n",
        "    print(f\"{i+1}) {line[:90]}{'...' if len(line) > 90 else ''}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfL4e9EPxWUb",
        "outputId": "619e6d81-0049-486f-cfc5-f8e0ca905c39"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences collected : 50,000\n",
            "\n",
            "Saved to: corpus_large.txt\n",
            "\n",
            "Sample sentences from CC-100\n",
            "1) ‘±’∫’°’£’°’≤’∏÷Ç’©’°÷Å’∏÷Ç’¥. 2-÷Ä’§ ’¥’°’Ω. ’é’°÷Ä’∏÷Ç’™’°’∂ ‘±’æ’•’ø’´’Ω’µ’°’∂’´ ’∞’∏’§’æ’°’Æ’®\n",
            "2) Nov 24, 2017 Comments Off on ‘±’∫’°’£’°’≤’∏÷Ç’©’°÷Å’∏÷Ç’¥. 2-÷Ä’§ ’¥’°’Ω. ’é’°÷Ä’∏÷Ç’™’°’∂ ‘±’æ’•’ø’´’Ω’µ’°’∂’´ ’∞’∏’§’æ’°’Æ’® ’Ä’°’µ’•’¨’´\n",
            "3) ’Ñ’°’Ω 2-÷Ä’§ (’Ñ’°’Ω 1-’´’∂’®’ù ’°’µ’Ω’ø’•’≤ )\n",
            "4) ‘±’∫’°’£’°’≤’∏÷Ç’©’°÷Å’∏÷Ç’¥-Armexit-’´ ’£’∏÷Ä’Æ’®’∂’©’°÷Å’∂ ’Ω’Ø’Ω’•’¨’∏÷Ç ’∞’°’¥’°÷Ä ’∂’°’≠ ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß ’Ω’ø’•’≤’Æ’•’¨ ’∞’°’µ’Ø’°’Ø’°’∂ ’Ω’∏÷Ç’¢’µ’•’Ø...\n",
            "5) ’Ñ’•÷Ä ’∫’°÷Ä’°’£’°’µ’∏÷Ç’¥ ’£’°’≤’∏÷Ç’©’°’µ’´’∂ ’æ’°÷Ä’π’°’Ø’°’¶’¥’´’∂ ’∞’∂’°÷Ä’°’æ’∏÷Ä ’ß ’∞’•’º’°÷Å’∂’•’¨ ’¥’´’°’µ’∂ ’™’∏’≤’∏’æ÷Ä’§’°’µ’´’∂ ’®’∂’§’æ’¶’¥’°’∂ ’¥’´’ª’∏÷Å...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Large SentencePiece BPE Model"
      ],
      "metadata": {
        "id": "fBfV2VVmx4hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    input=LARGE_CORPUS,\n",
        "    model_prefix='hy_bpe_large', # output: hy_bpe_large.model + hy_bpe_large.vocab\n",
        "    vocab_size=8000, # in small was only 300\n",
        "    model_type='bpe', # Byte Pair Encoding\n",
        "    character_coverage=1.0,  # cover 100% of Armenian Unicode characters\n",
        "    pad_id=0, # <pad> Padding token\n",
        "    unk_id=1, # <unk> Unknown token\n",
        "    bos_id=2, # <s> Beginning Of Sentence\n",
        "    eos_id=3, # </s> End Of Sentence\n",
        ")\n",
        "\n",
        "print(\"Generated files: hy_bpe_large.model,  hy_bpe_large.vocab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqmizAcxonp0",
        "outputId": "7b9c65bf-dcc0-48d8-a121-24a7fb8aa626"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated files: hy_bpe_large.model,  hy_bpe_large.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load large model"
      ],
      "metadata": {
        "id": "mRJE3KOPzQW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_large = spm.SentencePieceProcessor()\n",
        "sp_large.load('hy_bpe_large.model')\n",
        "\n",
        "large_vocab_size = sp_large.get_piece_size()\n",
        "print(f\"\\nLarge model vocabulary size: {large_vocab_size:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf7Qcv16zJWJ",
        "outputId": "9c3e2236-03ca-4726-b95e-08a29fc9e3f9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Large model vocabulary size: 8,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect first and last 30 entries"
      ],
      "metadata": {
        "id": "0c54M93TzTmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst 30 vocabulary entries (large model)\")\n",
        "print(f\"  {'ID':>5}  {'Token'}\\n\")\n",
        "\n",
        "for i in range(30):\n",
        "    print(f\"  {i:>5}  {sp_large.id_to_piece(i)}\")\n",
        "\n",
        "print(\"\\nLast 30 vocabulary entries (large model)\")\n",
        "print(f\"{'ID':>5}  {'Token'}\\n\")\n",
        "\n",
        "for i in range(large_vocab_size - 30, large_vocab_size):\n",
        "    print(f\"{i:>5}  {sp_large.id_to_piece(i)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oouIKHCRzOeS",
        "outputId": "ef73b41f-ab99-41b8-ecb0-7c72190eb80a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 30 vocabulary entries (large model)\n",
            "     ID  Token\n",
            "\n",
            "      0  <pad>\n",
            "      1  <unk>\n",
            "      2  <s>\n",
            "      3  </s>\n",
            "      4  ’∏÷Ç\n",
            "      5  ’°’∂\n",
            "      6  ’°÷Ä\n",
            "      7  ’•÷Ä\n",
            "      8  ’°’Ø\n",
            "      9  ’°’µ\n",
            "     10  ‚ñÅ’∞\n",
            "     11  ’´’∂\n",
            "     12  ’∏÷Ä\n",
            "     13  ’∏÷Ç’¥\n",
            "     14  ’•’¨\n",
            "     15  ’∏÷Ç’©\n",
            "     16  ’∂’•÷Ä\n",
            "     17  ‚ñÅ’ß\n",
            "     18  ’∏÷Ç’©’µ\n",
            "     19  ‚ñÅ’¥\n",
            "     20  ’°’¥\n",
            "     21  ’•’∂\n",
            "     22  ’∏÷Ç’∂\n",
            "     23  ‚ñÅ’Ø\n",
            "     24  ’°’ø\n",
            "     25  ’°’Ω\n",
            "     26  ’•÷Ç\n",
            "     27  ’°’æ\n",
            "     28  ’°’Ø’°’∂\n",
            "     29  ‚ñÅ’∂\n",
            "\n",
            "Last 30 vocabulary entries (large model)\n",
            "   ID  Token\n",
            "\n",
            " 7970  Â±±\n",
            " 7971  Â¥é\n",
            " 7972  ÊÉÖ\n",
            " 7973  Êîø\n",
            " 7974  Êï∞\n",
            " 7975  Êñá\n",
            " 7976  Êùë\n",
            " 7977  Ê≠¶\n",
            " 7978  Á¥∞\n",
            " 7979  Ëîµ\n",
            " 7980  Ë°å\n",
            " 7981  Ë©≥\n",
            " 7982  Èáé\n",
            " 7983  Í∏∞\n",
            " 7984  ÍπÄ\n",
            " 7985  Î≤î\n",
            " 7986  ÏÑú\n",
            " 7987  Ïö∏\n",
            " 7988  Ï∞Ω\n",
            " 7989  ÌÇ§\n",
            " 7990  Ìèâ\n",
            " 7991  ÔÄ≠\n",
            " 7992  Ô∏è\n",
            " 7993  üåπ\n",
            " 7994  üéÅ\n",
            " 7995  üíù\n",
            " 7996  üíü\n",
            " 7997  üì¢\n",
            " 7998  üòÄ\n",
            " 7999  üòâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode and Decode 5 Sentences"
      ],
      "metadata": {
        "id": "ilas-2uqzon8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Armenian sentences chosen to cover different topics and word lengths\n",
        "my_sentences = [\n",
        "    \"’Ä’°’µ’°’Ω’ø’°’∂’® ’£’•’≤’•÷Å’´’Ø ’•÷Ä’Ø’´÷Ä ’ß ‘ø’∏’æ’Ø’°’Ω’∏÷Ç’¥÷â\", # Geography\n",
        "    \"‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ÷É’∏’≠’∏÷Ç’¥ ’ß ’°’∑’≠’°÷Ä’∞’®÷â\", # Technology / AI\n",
        "    \"‘µ’Ω ’Ω’´÷Ä’∏÷Ç’¥ ’•’¥ ’∞’°’µ’Ø’°’Ø’°’∂ ’•÷Ä’°’™’∑’ø’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\", # Culture / personal\n",
        "    \"‘≥’´’ø’∏÷Ç’©’µ’∏÷Ç’∂’® ’¥’°÷Ä’§’Ø’∏÷Ç’©’µ’°’∂’® ’∂’∏÷Ä ’∞’∂’°÷Ä’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ’ß ’¢’°÷Å’∏÷Ç’¥÷â\", # Science\n",
        "    \"‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ÷á ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\", # Language / identity\n",
        "    \"‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ’•÷Ç ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\", # Language / identity\n",
        "]\n",
        "\n",
        "for idx, sentence in enumerate(my_sentences, start=1):\n",
        "    print(f\"\\nSentence {idx}: {sentence}\")\n",
        "\n",
        "    pieces = sp_large.encode(sentence, out_type=str)\n",
        "    ids    = sp_large.encode(sentence, out_type=int)\n",
        "    decoded = sp_large.decode(ids)\n",
        "    match  = (decoded == sentence)\n",
        "\n",
        "    print(f\"Token pieces: {pieces}\")\n",
        "    print(f\"Token IDs: {ids}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(f\"Match original: {match}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBUNYGZgozGS",
        "outputId": "df49f425-609a-436c-ec7d-12c7869d28f3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1: ’Ä’°’µ’°’Ω’ø’°’∂’® ’£’•’≤’•÷Å’´’Ø ’•÷Ä’Ø’´÷Ä ’ß ‘ø’∏’æ’Ø’°’Ω’∏÷Ç’¥÷â\n",
            "Token pieces: ['‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂’®', '‚ñÅ’£’•’≤’•÷Å’´’Ø', '‚ñÅ’•÷Ä’Ø’´÷Ä', '‚ñÅ’ß', '‚ñÅ‘ø’∏’æ’Ø’°’Ω', '’∏÷Ç’¥', '÷â']\n",
            "Token IDs: [1411, 2879, 2006, 17, 6203, 13, 7578]\n",
            "Decoded: ’Ä’°’µ’°’Ω’ø’°’∂’® ’£’•’≤’•÷Å’´’Ø ’•÷Ä’Ø’´÷Ä ’ß ‘ø’∏’æ’Ø’°’Ω’∏÷Ç’¥÷â\n",
            "Match original: True\n",
            "\n",
            "Sentence 2: ‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ÷É’∏’≠’∏÷Ç’¥ ’ß ’°’∑’≠’°÷Ä’∞’®÷â\n",
            "Token pieces: ['‚ñÅ‘±÷Ä', '’∞', '’•’Ω’ø', '’°’Ø’°’∂', '‚ñÅ’¢’°’∂', '’°’Ø’°’∂', '’∏÷Ç’©’µ’∏÷Ç’∂’®', '‚ñÅ÷É’∏’≠', '’∏÷Ç’¥', '‚ñÅ’ß', '‚ñÅ’°’∑’≠’°÷Ä’∞’®', '÷â']\n",
            "Token IDs: [182, 7537, 814, 28, 551, 28, 131, 370, 13, 17, 5409, 7578]\n",
            "Decoded: ‘±÷Ä’∞’•’Ω’ø’°’Ø’°’∂ ’¢’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ÷É’∏’≠’∏÷Ç’¥ ’ß ’°’∑’≠’°÷Ä’∞’®÷â\n",
            "Match original: True\n",
            "\n",
            "Sentence 3: ‘µ’Ω ’Ω’´÷Ä’∏÷Ç’¥ ’•’¥ ’∞’°’µ’Ø’°’Ø’°’∂ ’•÷Ä’°’™’∑’ø’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\n",
            "Token pieces: ['‚ñÅ‘µ’Ω', '‚ñÅ’Ω’´÷Ä’∏÷Ç’¥', '‚ñÅ’•’¥', '‚ñÅ’∞’°’µ’Ø’°’Ø’°’∂', '‚ñÅ’•÷Ä’°’™’∑’ø', '’∏÷Ç’©’µ’∏÷Ç’∂’®', '÷â']\n",
            "Token IDs: [1121, 2778, 288, 1033, 2713, 131, 7578]\n",
            "Decoded: ‘µ’Ω ’Ω’´÷Ä’∏÷Ç’¥ ’•’¥ ’∞’°’µ’Ø’°’Ø’°’∂ ’•÷Ä’°’™’∑’ø’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\n",
            "Match original: True\n",
            "\n",
            "Sentence 4: ‘≥’´’ø’∏÷Ç’©’µ’∏÷Ç’∂’® ’¥’°÷Ä’§’Ø’∏÷Ç’©’µ’°’∂’® ’∂’∏÷Ä ’∞’∂’°÷Ä’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ’ß ’¢’°÷Å’∏÷Ç’¥÷â\n",
            "Token pieces: ['‚ñÅ‘≥’´’ø', '’∏÷Ç’©’µ’∏÷Ç’∂’®', '‚ñÅ’¥’°÷Ä’§’Ø', '’∏÷Ç’©’µ’°’∂’®', '‚ñÅ’∂’∏÷Ä', '‚ñÅ’∞’∂’°÷Ä’°’æ’∏÷Ä', '’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä', '‚ñÅ’ß', '‚ñÅ’¢’°÷Å', '’∏÷Ç’¥', '÷â']\n",
            "Token IDs: [2623, 131, 865, 458, 304, 573, 224, 17, 371, 13, 7578]\n",
            "Decoded: ‘≥’´’ø’∏÷Ç’©’µ’∏÷Ç’∂’® ’¥’°÷Ä’§’Ø’∏÷Ç’©’µ’°’∂’® ’∂’∏÷Ä ’∞’∂’°÷Ä’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ’ß ’¢’°÷Å’∏÷Ç’¥÷â\n",
            "Match original: True\n",
            "\n",
            "Sentence 5: ‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ÷á ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\n",
            "Token pieces: ['‚ñÅ‘º', '’•’¶', '’∏÷Ç’∂', '‚ñÅ’™’∏’≤’∏’æ÷Ä’§’´', '‚ñÅ’∞’∏’£', '’´’∂', '‚ñÅ’ß', '‚ñÅ’•÷Ç', '‚ñÅ’´’∂÷Ñ’∂', '’∏÷Ç’©’µ’°’∂', '‚ñÅ’∞’´’¥', '÷Ñ’®', '÷â']\n",
            "Token IDs: [226, 297, 22, 2177, 918, 11, 17, 48, 845, 45, 338, 289, 7578]\n",
            "Decoded: ‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ’•÷Ç ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\n",
            "Match original: False\n",
            "\n",
            "Sentence 6: ‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ’•÷Ç ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\n",
            "Token pieces: ['‚ñÅ‘º', '’•’¶', '’∏÷Ç’∂', '‚ñÅ’™’∏’≤’∏’æ÷Ä’§’´', '‚ñÅ’∞’∏’£', '’´’∂', '‚ñÅ’ß', '‚ñÅ’•÷Ç', '‚ñÅ’´’∂÷Ñ’∂', '’∏÷Ç’©’µ’°’∂', '‚ñÅ’∞’´’¥', '÷Ñ’®', '÷â']\n",
            "Token IDs: [226, 297, 22, 2177, 918, 11, 17, 48, 845, 45, 338, 289, 7578]\n",
            "Decoded: ‘º’•’¶’∏÷Ç’∂ ’™’∏’≤’∏’æ÷Ä’§’´ ’∞’∏’£’´’∂ ’ß ’•÷Ç ’´’∂÷Ñ’∂’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’®÷â\n",
            "Match original: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare Average Tokens per Sentence\n",
        "\n",
        "Small model (vocab=300) vs Large model (vocab=8000)"
      ],
      "metadata": {
        "id": "yTmujRCw0Qo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the small model trained in Part 1\n",
        "sp_small = spm.SentencePieceProcessor()\n",
        "sp_small.load('hy_bpe.model')\n",
        "\n",
        "# Use the original small corpus as a shared benchmark\n",
        "with open('corpus.txt', encoding='utf-8') as f:\n",
        "    benchmark_sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "small_token_counts = []\n",
        "large_token_counts = []\n",
        "\n",
        "for sentence in benchmark_sentences:\n",
        "    small_tokens = sp_small.encode(sentence, out_type=str)\n",
        "    large_tokens = sp_large.encode(sentence, out_type=str)\n",
        "    small_token_counts.append(len(small_tokens))\n",
        "    large_token_counts.append(len(large_tokens))\n",
        "\n",
        "avg_small = sum(small_token_counts) / len(small_token_counts)\n",
        "avg_large = sum(large_token_counts) / len(large_token_counts)\n",
        "reduction = ((avg_small - avg_large) / avg_small) * 100\n",
        "\n",
        "print(f\"\\nCorpus: {len(benchmark_sentences)} sentences (corpus.txt)\")\n",
        "print(f\"\\n{'Model':<25}  {'Vocab Size':>10}  {'Avg Tokens/Sentence':>20}\\n\")\n",
        "print(f\"{'Small model (Part 1)':<25}  {sp_small.get_piece_size():>10,}  {avg_small:>20.2f}\")\n",
        "print(f\"{'Large model (Home Task)':<25}  {sp_large.get_piece_size():>10,}  {avg_large:>20.2f}\")\n",
        "print(f\"\\nToken reduction with large model: {reduction:.1f}%\")\n",
        "print(f\"(fewer tokens = more information packed per token = better efficiency)\")\n",
        "\n",
        "print(\"\\nExamples:\")\n",
        "for sentence in benchmark_sentences[:3]:\n",
        "    s_pieces = sp_small.encode(sentence, out_type=str)\n",
        "    l_pieces = sp_large.encode(sentence, out_type=str)\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"Small: {s_pieces}  ({len(s_pieces)} tokens)\")\n",
        "    print(f\"Large: {l_pieces}  ({len(l_pieces)} tokens)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1U2DlMlo19F",
        "outputId": "abd60a1a-aff6-4223-b431-55104ee5c477"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus: 93 sentences (corpus.txt)\n",
            "\n",
            "Model                      Vocab Size   Avg Tokens/Sentence\n",
            "\n",
            "Small model (Part 1)              300                 17.38\n",
            "Large model (Home Task)         8,000                 10.91\n",
            "\n",
            "Token reduction with large model: 37.2%\n",
            "(fewer tokens = more information packed per token = better efficiency)\n",
            "\n",
            "Examples:\n",
            "\n",
            "Sentence: ’Ä’°’µ’°’Ω’ø’°’∂’® ÷É’∏÷Ñ÷Ä, ’¢’°’µ÷Å ’∞’°÷Ä’∏÷Ç’Ω’ø ’∫’°’ø’¥’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ç’∂’•÷Å’∏’≤ ’•÷Ä’Ø’´÷Ä ’ß÷â\n",
            "Small: ['‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂', '’®', '‚ñÅ÷É', '’∏', '÷Ñ÷Ä', ',', '‚ñÅ’¢', '’°’µ', '÷Å', '‚ñÅ’∞’°÷Ä’∏÷Ç’Ω’ø', '‚ñÅ’∫', '’°’ø', '’¥', '’∏÷Ç’©’µ’∏÷Ç’∂', '‚ñÅ’∏÷Ç’∂', '’•÷Å', '’∏’≤', '‚ñÅ’•÷Ä', '’Ø', '’´÷Ä', '‚ñÅ’ß', '÷â']  (22 tokens)\n",
            "Large: ['‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂’®', '‚ñÅ÷É’∏÷Ñ÷Ä', ',', '‚ñÅ’¢’°’µ÷Å', '‚ñÅ’∞’°÷Ä’∏÷Ç’Ω’ø', '‚ñÅ’∫’°’ø’¥’∏÷Ç’©’µ’∏÷Ç’∂', '‚ñÅ’∏÷Ç’∂’•÷Å’∏’≤', '‚ñÅ’•÷Ä’Ø’´÷Ä', '‚ñÅ’ß', '÷â']  (10 tokens)\n",
            "\n",
            "Sentence: ‘µ÷Ä÷á’°’∂’® ’Ä’°’µ’°’Ω’ø’°’∂’´ ’¥’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ’∂ ’ß ÷á ’°’¥’•’∂’°’¥’•’Æ ÷Ñ’°’≤’°÷Ñ’®÷â\n",
            "Small: ['‚ñÅ‘µ÷Ä', '’•÷Ç', '’°’∂', '’®', '‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂’´', '‚ñÅ’¥', '’°’µ', '÷Ä’°', '÷Ñ', '’°’≤', '’°', '÷Ñ’∂', '‚ñÅ’ß', '‚ñÅ’•÷Ç', '‚ñÅ’°’¥’•’∂', '’°’¥’•’Æ', '‚ñÅ', '÷Ñ', '’°’≤', '’°', '÷Ñ', '’®', '÷â']  (23 tokens)\n",
            "Large: ['‚ñÅ‘µ÷Ä’•÷Ç’°’∂’®', '‚ñÅ’Ä’°’µ’°’Ω’ø’°’∂’´', '‚ñÅ’¥’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ', '’∂', '‚ñÅ’ß', '‚ñÅ’•÷Ç', '‚ñÅ’°’¥’•’∂’°’¥’•’Æ', '‚ñÅ÷Ñ’°’≤’°÷Ñ’®', '÷â']  (9 tokens)\n",
            "\n",
            "Sentence: ‘±÷Ä’°÷Ä’°’ø ’¨’•’º’® ’∞’°’µ ’™’∏’≤’∏’æ÷Ä’§’´ ’≠’∏÷Ä’∞÷Ä’§’°’∂’´’∑’∂ ’ß÷â\n",
            "Small: ['‚ñÅ‘±÷Ä', '’°÷Ä', '’°’ø', '‚ñÅ’¨', '’•', '’º', '’®', '‚ñÅ’∞’°’µ', '‚ñÅ’™’∏’≤’∏’æ÷Ä’§’´', '‚ñÅ’≠', '’∏÷Ä', '’∞', '÷Ä’§', '’°’∂', '’´’∑', '’∂', '‚ñÅ’ß', '÷â']  (18 tokens)\n",
            "Large: ['‚ñÅ‘±÷Ä’°÷Ä’°’ø', '‚ñÅ’¨’•’º', '’®', '‚ñÅ’∞’°’µ', '‚ñÅ’™’∏’≤’∏’æ÷Ä’§’´', '‚ñÅ’≠’∏÷Ä’∞÷Ä’§’°’∂’´’∑', '’∂', '‚ñÅ’ß', '÷â']  (9 tokens)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "Training on 50,000 CC-100 sentences (instead of 93 sentences) gives the BPE more information about which character sequences co-occur frequently\n",
        "in Armenian. As a result, the large model learns\n",
        "thousands of complete words and longer  combinations, whereas the small\n",
        "model breaking even common words into many\n",
        "fragments.\n",
        "\n",
        "It is visible in token efficiency: the large model produces\n",
        "fewer tokens per sentence on average. Fewer tokens means each\n",
        "token carries more meaning, which reduces the sequence length.\n",
        "\n",
        "Armenian is a rich language with many inflectional suffixes (\n",
        "\"-’∏÷Ç’¥\", \"-’∏÷Ç’©’µ’∏÷Ç’∂\"). With only 300 tokens the small model splits words more.\n",
        "The large model, has seen suffixes\n",
        "thousands of times and merges them early, producing meaningful\n",
        "subword units.\n",
        "\n",
        "In examples we can see that \"’¥’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ’∂\" word was spleted into 7 tokens by small model and only 2 tokens by large model. \"÷Ñ’°’≤’°÷Ñ’®\" word into 6 tokens by small one, and 1 token by big one. It's huge.\n",
        "\n",
        "On other hand, we also can see that both small and large models understand words like \"’∞’°’µ\" and \"’Ä’°’µ’°’Ω’ø’°’∂’´\" as one token."
      ],
      "metadata": {
        "id": "Kc5znmMs1a2O"
      }
    }
  ]
}