{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gensim scikit-learn nltk\n",
        "\n",
        "import os\n",
        "os.makedirs(\"src\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# src/train_lda.py\n",
        "# -----------------------------\n",
        "train_py = r'''\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "N_DOCS = 1000\n",
        "NUM_TOPICS = 10\n",
        "PASSES = 15\n",
        "\n",
        "CUSTOM_STOPWORDS = {\n",
        "    # generic conversation words in 20NG\n",
        "    \"people\",\"know\",\"like\",\"time\",\"said\",\"think\",\"use\",\"thanks\",\"year\",\"want\",\"good\",\n",
        "    # email/meta tokens\n",
        "    \"edu\",\"com\",\"subject\",\"organization\",\"writes\",\"article\",\"lines\",\"from\",\"re\",\n",
        "}\n",
        "\n",
        "def ensure_nltk() -> None:\n",
        "    nltk.download(\"wordnet\", quiet=True)\n",
        "    nltk.download(\"omw-1.4\", quiet=True)\n",
        "\n",
        "def preprocess(text: str, lemmatizer: WordNetLemmatizer) -> List[str]:\n",
        "    # tokenize + lowercase (gensim)\n",
        "    tokens = simple_preprocess(text, deacc=True, min_len=3, max_len=20)\n",
        "    # remove stopwords\n",
        "    stop = set(STOPWORDS) | CUSTOM_STOPWORDS\n",
        "    tokens = [t for t in tokens if t not in stop]\n",
        "    # lemmatize (reduces variations: games->game)\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "def main() -> None:\n",
        "    ensure_nltk()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    data = fetch_20newsgroups(subset=\"train\", remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "    docs_raw = data.data[:N_DOCS]\n",
        "\n",
        "    docs = [preprocess(d, lemmatizer) for d in docs_raw]\n",
        "\n",
        "    # bigrams (helps: \"space_shuttle\", \"gun_control\" type patterns)\n",
        "    phrases = Phrases(docs, min_count=5, threshold=10.0)\n",
        "    bigram = Phraser(phrases)\n",
        "    docs = [bigram[d] for d in docs]\n",
        "\n",
        "    dictionary = Dictionary(docs)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "    dictionary.compactify()\n",
        "\n",
        "    corpus = [dictionary.doc2bow(d) for d in docs]\n",
        "\n",
        "    lda = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=NUM_TOPICS,\n",
        "        passes=PASSES,\n",
        "        alpha=\"auto\",\n",
        "        eta=\"auto\",\n",
        "        random_state=42,\n",
        "        eval_every=None,\n",
        "    )\n",
        "\n",
        "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "    lda.save(os.path.join(MODELS_DIR, \"lda_model\"))\n",
        "    dictionary.save(os.path.join(MODELS_DIR, \"lda_dictionary.dict\"))\n",
        "\n",
        "    print(\"\\n=== Discovered Topics (Top 15 words) ===\")\n",
        "    for tid, tstr in lda.print_topics(num_topics=NUM_TOPICS, num_words=15):\n",
        "        print(f\"Topic {tid}: {tstr}\")\n",
        "\n",
        "    print(f\"\\nSaved model: {MODELS_DIR}/lda_model\")\n",
        "    print(f\"Saved dictionary: {MODELS_DIR}/lda_dictionary.dict\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "open(\"src/train_lda.py\",\"w\",encoding=\"utf-8\").write(train_py)\n",
        "\n",
        "# -----------------------------\n",
        "# src/label_topics.py\n",
        "# -----------------------------\n",
        "label_py = r'''\n",
        "import os\n",
        "import json\n",
        "from typing import Dict\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "def main() -> None:\n",
        "    lda = LdaModel.load(os.path.join(MODELS_DIR, \"lda_model\"))\n",
        "    _dictionary = Dictionary.load(os.path.join(MODELS_DIR, \"lda_dictionary.dict\"))\n",
        "\n",
        "    labels: Dict[str, str] = {}\n",
        "\n",
        "    print(\"\\n=== Topics to Label (Top 20 words with probabilities) ===\")\n",
        "    for topic_id in range(lda.num_topics):\n",
        "        terms = lda.show_topic(topic_id, topn=20)\n",
        "        pretty = \", \".join([f\"{w} ({p:.3f})\" for w, p in terms])\n",
        "        print(f\"\\nTopic {topic_id}:\\n  {pretty}\")\n",
        "\n",
        "        name = input(\"Enter a meaningful topic name (Enter to keep default): \").strip()\n",
        "        labels[str(topic_id)] = name if name else f\"Topic {topic_id}\"\n",
        "\n",
        "    labels_path = os.path.join(MODELS_DIR, \"topic_labels.json\")\n",
        "    with open(labels_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(labels, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\n=== Final Topic Label Summary ===\")\n",
        "    for k in sorted(labels.keys(), key=lambda x: int(x)):\n",
        "        print(f\"{k}: {labels[k]}\")\n",
        "    print(f\"\\nSaved labels to: {labels_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "open(\"src/label_topics.py\",\"w\",encoding=\"utf-8\").write(label_py)\n",
        "\n",
        "# -----------------------------\n",
        "# src/infer_topics.py\n",
        "# -----------------------------\n",
        "infer_py = r'''\n",
        "import os\n",
        "import json\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "CUSTOM_STOPWORDS = {\n",
        "    \"people\",\"know\",\"like\",\"time\",\"said\",\"think\",\"use\",\"thanks\",\"year\",\"want\",\"good\",\n",
        "    \"edu\",\"com\",\"subject\",\"organization\",\"writes\",\"article\",\"lines\",\"from\",\"re\",\n",
        "}\n",
        "\n",
        "def ensure_nltk() -> None:\n",
        "    nltk.download(\"wordnet\", quiet=True)\n",
        "    nltk.download(\"omw-1.4\", quiet=True)\n",
        "\n",
        "def preprocess(text: str, lemmatizer: WordNetLemmatizer) -> List[str]:\n",
        "    tokens = simple_preprocess(text, deacc=True, min_len=3, max_len=20)\n",
        "    stop = set(STOPWORDS) | CUSTOM_STOPWORDS\n",
        "    tokens = [t for t in tokens if t not in stop]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "def load_labels(num_topics: int) -> Dict[int, str]:\n",
        "    path = os.path.join(MODELS_DIR, \"topic_labels.json\")\n",
        "    labels: Dict[int, str] = {}\n",
        "    if os.path.exists(path):\n",
        "        raw = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
        "        labels = {int(k): str(v) for k, v in raw.items()}\n",
        "    for i in range(num_topics):\n",
        "        labels.setdefault(i, f\"Topic {i}\")\n",
        "    return labels\n",
        "\n",
        "def top_words(lda: LdaModel, topic_id: int, topn: int = 5) -> str:\n",
        "    return \", \".join([w for w, _p in lda.show_topic(topic_id, topn=topn)])\n",
        "\n",
        "def classify(text: str, lda: LdaModel, dictionary: Dictionary, lemmatizer: WordNetLemmatizer) -> List[Tuple[int, float]]:\n",
        "    bow = dictionary.doc2bow(preprocess(text, lemmatizer))\n",
        "    dist = lda.get_document_topics(bow, minimum_probability=0.0)\n",
        "    dist = sorted(dist, key=lambda x: x[1], reverse=True)[:3]\n",
        "    return dist\n",
        "\n",
        "def main() -> None:\n",
        "    ensure_nltk()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    lda = LdaModel.load(os.path.join(MODELS_DIR, \"lda_model\"))\n",
        "    dictionary = Dictionary.load(os.path.join(MODELS_DIR, \"lda_dictionary.dict\"))\n",
        "    labels = load_labels(lda.num_topics)\n",
        "\n",
        "    print(\"\\n=== Loaded Topics Summary (Top 5 words) ===\")\n",
        "    for tid in range(lda.num_topics):\n",
        "        print(f\"- {tid}: {labels[tid]} | {top_words(lda, tid, topn=5)}\")\n",
        "\n",
        "    samples = [\n",
        "        \"The new graphics card delivers amazing performance for gaming. The GPU can handle 4K resolution easily with ray tracing enabled. Gamers will love the improved frame rates.\",\n",
        "        \"Scientists discovered a new exoplanet orbiting a distant star in the habitable zone. The research team published their findings in Nature journal. This discovery could provide insights into planetary formation.\",\n",
        "        \"The basketball team won the championship after an incredible final game. The players celebrated with fans in the stadium. It was the team's first title in twenty years.\",\n",
        "        \"Congress passed a new bill regarding healthcare reform. The president is expected to sign the legislation next week. The policy will affect millions of citizens across the country.\",\n",
        "        \"I love cooking Italian food at home. Pasta carbonara and margherita pizza are my favorite dishes to make. Fresh ingredients make all the difference in authentic recipes.\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\\n=== Running 5 sample classifications ===\")\n",
        "    for s in samples:\n",
        "        preview = (s[:200] + \"...\") if len(s) > 200 else s\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Document preview:\")\n",
        "        print(preview)\n",
        "\n",
        "        top3 = classify(s, lda, dictionary, lemmatizer)\n",
        "        print(\"\\nTop 3 topics:\")\n",
        "        for tid, prob in top3:\n",
        "            print(f\"- {labels[tid]} (Topic {tid}) | P={prob:.4f} | top words: {top_words(lda, tid, topn=5)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "open(\"src/infer_topics.py\",\"w\",encoding=\"utf-8\").write(infer_py)\n",
        "\n",
        "print(\" Created: src/train_lda.py, src/label_topics.py, src/infer_topics.py\")\n",
        "\n",
        "# Run in required order\n",
        "!python src/train_lda.py\n",
        "!python src/label_topics.py\n",
        "!python src/infer_topics.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyOImVDvyEDu",
        "outputId": "5a80b7f5-f626-46d8-dd6c-d26612211628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created: src/train_lda.py, src/label_topics.py, src/infer_topics.py\n",
            "\n",
            "=== Discovered Topics (Top 15 words) ===\n",
            "Topic 0: 0.019*\"armenian\" + 0.008*\"government\" + 0.007*\"greek\" + 0.006*\"state\" + 0.006*\"case\" + 0.006*\"turkish\" + 0.005*\"killed\" + 0.005*\"health\" + 0.005*\"source\" + 0.005*\"person\" + 0.004*\"genocide\" + 0.004*\"word\" + 0.004*\"year\" + 0.004*\"russian\" + 0.004*\"day\"\n",
            "Topic 1: 0.013*\"space\" + 0.011*\"mission\" + 0.009*\"shuttle\" + 0.007*\"orbit\" + 0.007*\"nasa\" + 0.007*\"launch\" + 0.006*\"pitcher\" + 0.006*\"flight\" + 0.006*\"bike\" + 0.006*\"satellite\" + 0.006*\"league\" + 0.005*\"cost\" + 0.005*\"earth\" + 0.005*\"new\" + 0.005*\"better\"\n",
            "Topic 2: 0.017*\"jesus\" + 0.013*\"god\" + 0.009*\"argument\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"christian\" + 0.007*\"matthew\" + 0.006*\"true\" + 0.006*\"come\" + 0.006*\"father\" + 0.006*\"example\" + 0.006*\"believe\" + 0.005*\"course\" + 0.005*\"belief\" + 0.005*\"man\"\n",
            "Topic 3: 0.008*\"nasa\" + 0.007*\"application\" + 0.007*\"program\" + 0.007*\"information\" + 0.006*\"member\" + 0.006*\"muslim\" + 0.006*\"government\" + 0.006*\"armenian\" + 0.005*\"degree\" + 0.005*\"security\" + 0.004*\"nazi\" + 0.004*\"president\" + 0.004*\"center\" + 0.004*\"military\" + 0.004*\"soviet_armenia\"\n",
            "Topic 4: 0.010*\"read\" + 0.009*\"problem\" + 0.008*\"book\" + 0.007*\"new\" + 0.007*\"address\" + 0.007*\"israel\" + 0.007*\"error\" + 0.006*\"need\" + 0.006*\"battery\" + 0.006*\"period\" + 0.006*\"widget\" + 0.005*\"mark\" + 0.005*\"thing\" + 0.005*\"issue\" + 0.005*\"fit\"\n",
            "Topic 5: 0.012*\"bit\" + 0.009*\"problem\" + 0.009*\"machine\" + 0.009*\"card\" + 0.008*\"window\" + 0.008*\"memory\" + 0.008*\"speed\" + 0.008*\"scsi\" + 0.006*\"need\" + 0.006*\"drive\" + 0.006*\"software\" + 0.006*\"lot\" + 0.006*\"simms\" + 0.006*\"ram\" + 0.005*\"mac\"\n",
            "Topic 6: 0.013*\"car\" + 0.012*\"excellent\" + 0.008*\"rate\" + 0.008*\"new\" + 0.007*\"insurance\" + 0.005*\"great\" + 0.005*\"info\" + 0.005*\"missing\" + 0.005*\"sure\" + 0.005*\"scope\" + 0.005*\"period\" + 0.005*\"game\" + 0.004*\"cover\" + 0.004*\"sin\" + 0.004*\"treatment\"\n",
            "Topic 7: 0.026*\"max\" + 0.020*\"file\" + 0.013*\"image\" + 0.011*\"game\" + 0.010*\"program\" + 0.008*\"work\" + 0.006*\"format\" + 0.006*\"send\" + 0.006*\"disk\" + 0.006*\"product\" + 0.005*\"directory\" + 0.005*\"email\" + 0.005*\"university\" + 0.005*\"way\" + 0.004*\"hockey\"\n",
            "Topic 8: 0.013*\"key\" + 0.011*\"gun\" + 0.010*\"problem\" + 0.008*\"state\" + 0.007*\"right\" + 0.006*\"got\" + 0.006*\"number\" + 0.006*\"say\" + 0.005*\"need\" + 0.005*\"came\" + 0.005*\"way\" + 0.004*\"law\" + 0.004*\"mean\" + 0.004*\"point\" + 0.004*\"package\"\n",
            "Topic 9: 0.011*\"window\" + 0.009*\"run\" + 0.008*\"team\" + 0.008*\"game\" + 0.006*\"problem\" + 0.006*\"believe\" + 0.006*\"win\" + 0.006*\"better\" + 0.005*\"god\" + 0.005*\"yes\" + 0.005*\"fact\" + 0.005*\"christian\" + 0.004*\"morality\" + 0.004*\"look\" + 0.004*\"come\"\n",
            "\n",
            "Saved model: models/lda_model\n",
            "Saved dictionary: models/lda_dictionary.dict\n",
            "\n",
            "=== Topics to Label (Top 20 words with probabilities) ===\n",
            "\n",
            "Topic 0:\n",
            "  armenian (0.019), government (0.008), greek (0.007), state (0.006), case (0.006), turkish (0.006), killed (0.005), health (0.005), source (0.005), person (0.005), genocide (0.004), word (0.004), year (0.004), russian (0.004), day (0.004), palestinian (0.004), user (0.004), april (0.004), israel (0.004), land (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Ethnic Conflict & Genocide Politics\n",
            "\n",
            "Topic 1:\n",
            "  space (0.013), mission (0.011), shuttle (0.009), orbit (0.007), nasa (0.007), launch (0.007), pitcher (0.006), flight (0.006), bike (0.006), satellite (0.006), league (0.006), cost (0.005), earth (0.005), new (0.005), better (0.005), option (0.004), power (0.004), human (0.004), mon (0.004), atmosphere (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Space Exploration & Aerospace Missions\n",
            "\n",
            "Topic 2:\n",
            "  jesus (0.017), god (0.013), argument (0.009), thing (0.008), way (0.007), christian (0.007), matthew (0.007), true (0.006), come (0.006), father (0.006), example (0.006), believe (0.006), course (0.005), belief (0.005), man (0.005), word (0.005), truth (0.005), tell (0.005), day (0.005), mean (0.005)\n",
            "Enter a meaningful topic name (Enter to keep default): Christian Theology & Religious Debate\n",
            "\n",
            "Topic 3:\n",
            "  nasa (0.008), application (0.007), program (0.007), information (0.007), member (0.006), muslim (0.006), government (0.006), armenian (0.006), degree (0.005), security (0.005), nazi (0.004), president (0.004), center (0.004), military (0.004), soviet_armenia (0.004), following (0.004), requirement (0.004), today (0.004), list (0.004), window (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Government, Security & International Affairs\n",
            "\n",
            "Topic 4:\n",
            "  read (0.010), problem (0.009), book (0.008), new (0.007), address (0.007), israel (0.007), error (0.007), need (0.006), battery (0.006), period (0.006), widget (0.006), mark (0.005), thing (0.005), issue (0.005), fit (0.005), day (0.005), liar (0.004), run (0.004), look (0.004), little (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): General Issues, Problems & Troubleshooting\n",
            "\n",
            "Topic 5:\n",
            "  bit (0.012), problem (0.009), machine (0.009), card (0.009), window (0.008), memory (0.008), speed (0.008), scsi (0.008), need (0.006), drive (0.006), software (0.006), lot (0.006), simms (0.006), ram (0.006), mac (0.005), car (0.005), case (0.005), disk (0.005), chip (0.005), thing (0.005)\n",
            "Enter a meaningful topic name (Enter to keep default): Computer Hardware, Memory & System Performance\n",
            "\n",
            "Topic 6:\n",
            "  car (0.013), excellent (0.012), rate (0.008), new (0.008), insurance (0.007), great (0.005), info (0.005), missing (0.005), sure (0.005), scope (0.005), period (0.005), game (0.005), cover (0.004), sin (0.004), treatment (0.004), cost (0.004), effect (0.004), poster (0.004), mile (0.004), look (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Automobiles, Insurance & Consumer Costs\n",
            "\n",
            "Topic 7:\n",
            "  max (0.026), file (0.020), image (0.013), game (0.011), program (0.010), work (0.008), format (0.006), send (0.006), disk (0.006), product (0.006), directory (0.005), email (0.005), university (0.005), way (0.005), hockey (0.004), idea (0.004), problem (0.004), screen (0.004), gif (0.004), version (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Digital Media, Files & Software Applications\n",
            "\n",
            "Topic 8:\n",
            "  key (0.013), gun (0.011), problem (0.010), state (0.008), right (0.007), got (0.006), number (0.006), say (0.006), need (0.005), came (0.005), way (0.005), law (0.004), mean (0.004), point (0.004), package (0.004), going (0.004), motif (0.004), open (0.004), hand (0.004), weapon (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Gun Rights, Law & Public Policy\n",
            "\n",
            "Topic 9:\n",
            "  window (0.011), run (0.009), team (0.008), game (0.008), problem (0.006), believe (0.006), win (0.006), better (0.006), god (0.005), yes (0.005), fact (0.005), christian (0.005), morality (0.004), look (0.004), come (0.004), moral (0.004), man (0.004), religion (0.004), brian (0.004), bios (0.004)\n",
            "Enter a meaningful topic name (Enter to keep default): Sports Competition, Beliefs & Moral Debate\n",
            "\n",
            "=== Final Topic Label Summary ===\n",
            "0: Ethnic Conflict & Genocide Politics\n",
            "1: Space Exploration & Aerospace Missions\n",
            "2: Christian Theology & Religious Debate\n",
            "3: Government, Security & International Affairs\n",
            "4: General Issues, Problems & Troubleshooting\n",
            "5: Computer Hardware, Memory & System Performance\n",
            "6: Automobiles, Insurance & Consumer Costs\n",
            "7: Digital Media, Files & Software Applications\n",
            "8: Gun Rights, Law & Public Policy\n",
            "9: Sports Competition, Beliefs & Moral Debate\n",
            "\n",
            "Saved labels to: models/topic_labels.json\n",
            "\n",
            "=== Loaded Topics Summary (Top 5 words) ===\n",
            "- 0: Ethnic Conflict & Genocide Politics | armenian, government, greek, state, case\n",
            "- 1: Space Exploration & Aerospace Missions | space, mission, shuttle, orbit, nasa\n",
            "- 2: Christian Theology & Religious Debate | jesus, god, argument, thing, way\n",
            "- 3: Government, Security & International Affairs | nasa, application, program, information, member\n",
            "- 4: General Issues, Problems & Troubleshooting | read, problem, book, new, address\n",
            "- 5: Computer Hardware, Memory & System Performance | bit, problem, machine, card, window\n",
            "- 6: Automobiles, Insurance & Consumer Costs | car, excellent, rate, new, insurance\n",
            "- 7: Digital Media, Files & Software Applications | max, file, image, game, program\n",
            "- 8: Gun Rights, Law & Public Policy | key, gun, problem, state, right\n",
            "- 9: Sports Competition, Beliefs & Moral Debate | window, run, team, game, problem\n",
            "\n",
            "\n",
            "=== Running 5 sample classifications ===\n",
            "\n",
            "================================================================================\n",
            "Document preview:\n",
            "The new graphics card delivers amazing performance for gaming. The GPU can handle 4K resolution easily with ray tracing enabled. Gamers will love the improved frame rates.\n",
            "\n",
            "Top 3 topics:\n",
            "- Computer Hardware, Memory & System Performance (Topic 5) | P=0.5255 | top words: bit, problem, machine, card, window\n",
            "- Sports Competition, Beliefs & Moral Debate (Topic 9) | P=0.2353 | top words: window, run, team, game, problem\n",
            "- Automobiles, Insurance & Consumer Costs (Topic 6) | P=0.2072 | top words: car, excellent, rate, new, insurance\n",
            "\n",
            "================================================================================\n",
            "Document preview:\n",
            "Scientists discovered a new exoplanet orbiting a distant star in the habitable zone. The research team published their findings in Nature journal. This discovery could provide insights into planetary ...\n",
            "\n",
            "Top 3 topics:\n",
            "- Government, Security & International Affairs (Topic 3) | P=0.5096 | top words: nasa, application, program, information, member\n",
            "- Sports Competition, Beliefs & Moral Debate (Topic 9) | P=0.4515 | top words: window, run, team, game, problem\n",
            "- Computer Hardware, Memory & System Performance (Topic 5) | P=0.0063 | top words: bit, problem, machine, card, window\n",
            "\n",
            "================================================================================\n",
            "Document preview:\n",
            "The basketball team won the championship after an incredible final game. The players celebrated with fans in the stadium. It was the team's first title in twenty years.\n",
            "\n",
            "Top 3 topics:\n",
            "- Sports Competition, Beliefs & Moral Debate (Topic 9) | P=0.8345 | top words: window, run, team, game, problem\n",
            "- General Issues, Problems & Troubleshooting (Topic 4) | P=0.1226 | top words: read, problem, book, new, address\n",
            "- Computer Hardware, Memory & System Performance (Topic 5) | P=0.0069 | top words: bit, problem, machine, card, window\n",
            "\n",
            "================================================================================\n",
            "Document preview:\n",
            "Congress passed a new bill regarding healthcare reform. The president is expected to sign the legislation next week. The policy will affect millions of citizens across the country.\n",
            "\n",
            "Top 3 topics:\n",
            "- Ethnic Conflict & Genocide Politics (Topic 0) | P=0.5639 | top words: armenian, government, greek, state, case\n",
            "- Gun Rights, Law & Public Policy (Topic 8) | P=0.4029 | top words: key, gun, problem, state, right\n",
            "- Computer Hardware, Memory & System Performance (Topic 5) | P=0.0053 | top words: bit, problem, machine, card, window\n",
            "\n",
            "================================================================================\n",
            "Document preview:\n",
            "I love cooking Italian food at home. Pasta carbonara and margherita pizza are my favorite dishes to make. Fresh ingredients make all the difference in authentic recipes.\n",
            "\n",
            "Top 3 topics:\n",
            "- Sports Competition, Beliefs & Moral Debate (Topic 9) | P=0.6371 | top words: window, run, team, game, problem\n",
            "- Gun Rights, Law & Public Policy (Topic 8) | P=0.2896 | top words: key, gun, problem, state, right\n",
            "- Computer Hardware, Memory & System Performance (Topic 5) | P=0.0119 | top words: bit, problem, machine, card, window\n"
          ]
        }
      ]
    }
  ]
}